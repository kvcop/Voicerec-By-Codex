Date: 2025-09-27 01:16:03

# Анализ прогресса проекта Voicerec-By-Codex

## Соответствие текущей реализации плану

**Общая картина:** Проект находится на этапе реализованного каркаса системы - заложены основы **backend**, **frontend** и инфраструктуры. Однако ключевые функциональные компоненты (база данных, обработка аудио, бизнес-логика распознавания, полноценный UI и авторизация) пока не реализованы. План разработки подробно разбит на этапы, и фактическое состояние соответствует **началу Фазы 1** этого плана[\[1\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L36-L44)[\[2\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L170-L178). Ниже представлена таблица соответствия пунктов плана их текущему статусу:

| **Пункт плана** | **Статус** | **Комментарий** |
| --- | --- | --- |
| **Основа backend** (FastAPI приложение) | **Выполнен** | Проект структура создана[\[3\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L7-L15), приложение и роутеры инициализированы. |
| **Эндпоинт /upload** (прием аудио, запись в data/raw/) | **Выполнен** | Реализован, файл сохраняется на диск[\[4\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L24-L32)[\[5\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L34-L39) (с пометкой TODO о переносе в безопасное хранилище). |
| **Эндпоинт /stream/{meeting_id}** (SSE стриминг) | **Частично** | Маршрут создан[\[6\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L49-L54), но бизнес-логика заглушена - трансляция пустая (реализация TranscriptService.stream_transcript отсутствует[\[7\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L56-L64)). |
| **Эндпоинт /health** (проверка состояния) | **Выполнен** | Реализован простой ответ "status: ok"[\[8\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/backend/app/main.py#L12-L16). |
| **Система конфигурации** (Pydantic Settings, GPUSettings) | **Выполнен** | Класс настроек реализован, в т.ч. валидация TLS для GPU и префикс GPU_[\[9\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/backend/app/core/settings.py#L34-L42)[\[10\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/backend/app/core/settings.py#L43-L51). |
| **gRPC клиенты (моки)** - MockTranscribeClient, MockDiarizeClient, MockSummarizeClient и фабрика create_grpc_client | **Выполнен** | Заглушки реализованы для тестов[\[11\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L14-L19), возвращают фиксированные данные из JSON[\[12\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/grpc_client.py#L14-L23)[\[13\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/grpc_client.py#L41-L49). |
| **Proto-файлы** для сервисов (transcribe/diarize/summarize) | **Выполнен** | Определения gRPC протоколов созданы (файлы в каталоге protos/[\[14\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L18-L21)). |
| **Базовые тесты** (API и клиенты) | **Выполнен** | Присутствуют тесты для основных модулей (например, test_main.py, test_meeting.py, test_grpc_client.py)[\[14\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L18-L21). |
| **Безопасность GPU** (TLS для GPU-сервера) | **Выполнен** | Поддержка TLS настроена в конфигурации (проверка наличия сертификатов при grpc_use_tls=True[\[9\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/backend/app/core/settings.py#L34-L42)). Документ docs/gpu_security.md содержит инструкции. |
| **Frontend каркас** (React + Vite) | **Выполнен** | Инициализировано приложение на React (Vite), настроены сборка и тестирование (Vitest)[\[15\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L23-L31). |
| **Интернационализация (en/ru)** | **Выполнен** | Добавлены JSON-файлы локализации (англ/рус) и логика выбора языка (пример - хранение выбора в localStorage)[\[16\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L20-L23). |
| **Базовые компоненты UI** (App, Dialog) | **Выполнен** | Созданы простейшие компоненты интерфейса (пока без специфичного функционала). |
| **Настройка тестов фронтенда** (Vitest, RTL) | **Выполнен** | Vitest настроен (vitest.config.ts), базовые тесты (например, App.test.tsx) присутствуют. |
| **Docker-compose для mock GPU сервисов** | **Выполнен** | Добавлен файл infra/docker-compose.gpu.yml для поднятия локальных контейнеров (моки ASR, diarization, summary)[\[17\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/README.md#L22-L30). |
| **Документация (диаграммы, описания)** | **Частично** | Создан каркас документации (разделы docs/ и docs/ai-context/ со структурой)[\[18\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/docs/ai-context/project-structure.md#L128-L137). Имеются диаграммы (например, мермаид-схема архитектуры в QUESTIONS.md[\[19\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L86-L94)). Однако ряд документов-заглушек ждет наполнения (например, system-integration.md, handoff.md помечены как to be created[\[20\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/docs/ai-context/project-structure.md#L130-L137)). Актуализация документации потребуется по мере разработки. |
| **Скрипты установки зависимостей** (install_deps.sh, install_postgres.sh) | **Выполнен** | Реализованы сценарии автоматической установки Python/Node зависимостей и PostgreSQL (Postgres устанавливается с паролем password для тестов[\[21\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L3-L10)). |
| \--- **Критические компоненты backend (не реализованы):** --- |     |     |
| **База данных** (модели Meeting/Transcript/User, Alembic миграции, подключение к PostgreSQL) | **Не начат** | Код моделей и миграций отсутствует[\[1\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L36-L44). Пока используется только заглушка с AsyncSession (подключение к реальной БД не настроено). |
| **Реальная обработка аудио** (гранулярно: интеграция с GPU-сервисами вместо моков, генерация gRPC-клиентов из .proto, обработка соединений) | **Не начат** | В текущей реализации вызовы сервисов распознавания, диаризации, суммаризации не выполняются - используется заглушечный сервис. Требуется сгенерировать pb2 модули и реализовать вызыв реальных сервисов[\[22\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L42-L50)[\[23\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L16-L19). |
| **Бизнес-логика распознавания** (реализация TranscriptService.stream_transcript, ассинхронная обработка аудио, сохранение результатов в БД) | **Не начат** | Класс TranscriptService присутствует, но метод стриминга пустой[\[7\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L56-L64). Нет кода передачи аудио на распознавание и накопления результатов. Соответственно, транскрипции не сохраняются (отсутствует слой сохранения без БД). |
| **Аутентификация и авторизация** (система пользователей, JWT, защита эндпоинтов) | **Не начат** | Логика регистрации/входа, JWT-токены и middleware доступа пока не реализованы[\[24\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L52-L60). Вопросы интеграции с корпоративным SSO остаются открытыми. |
| \--- **Основной функционал frontend (не реализован):** --- |     |     |
| **UI загрузки аудио** (drag & drop, прогресс-бар, валидация файлов) | **Не начат** | Пока фронтенд не имеет отдельного компонента загрузки - требуется разработать с использованием библиотеки компонентов (планируется shadcn/ui). |
| **UI трансляции расшифровки** (отображение текста в реальном времени, подключение к SSE, индикация спикеров) | **Не начат** | На фронте отсутствует страница/компонент показа расшифровки встречи. Необходимо реализовать получение событий SSE с бэкенда и обновление текста, оформление с пометками говорящих. |
| **Управление встречами** (список записей пользователя, просмотр расшифровки, экспорт) | **Не начат** | В текущем UI нет раздела списка встреч и деталей по ним. Эти возможности планируются, но требуют сначала реализации БД и авторизации (привязка встреч к пользователям). |
| \--- **Безопасность/проц:** --- |     |     |
| **Безопасное хранилище аудио/транскриптов** (шифрование, миграция из data/raw/, резервное копирование) | **Не начат** | Пока файлы хранятся просто на диске[\[4\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L24-L32). Реализация защищенного хранилища отложена (дедлайн - конец 2025 г. по плану[\[25\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L74-L79)) и на текущий прогресс не повлияла. |

**Вывод по прогрессу:** На данный момент выполнены подготовительные работы и создан каркас системы. Это соответствует разделу "✅ Что уже реализовано" плана[\[3\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L7-L15)[\[26\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L16-L24). Критически важные элементы из раздела "❌ Что еще не реализовано" пока _не_ реализованы[\[1\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L36-L44)[\[27\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L46-L55), за исключением некоторых заделов (например, SSE-эндпоинт объявлен, класс сервиса создан, но без функционала). Таким образом, реализация отстает от плана на уровне начальных этапов Фазы 1.

## Полнота и корректность плана относительно целей проекта

**Цель проекта:** создать сервис голосового управления/распознавания (стенографирования) встреч с помощью локальных моделей (т.е. полностью офлайн-процесс: распознавание речи, идентификация говорящих, суммаризация обсуждений)[\[28\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L18-L26)[\[29\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L156-L158). План реализации в docs/implementation_plan.md в целом отражает эту цель: в фазах предусмотрены разработка распознавания речи (интеграция ASR), диаризации спикеров, вывод результата пользователю, а также последующие улучшения (безопасное хранение, масштабирование и т.д.)[\[30\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L140-L149)[\[31\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L198-L206).

**Соответствие ключевым функциям:**

- План последовательно охватывает:
- **Расшифровка речи (ASR)** - фаза 2 (реализация реальной обработки аудио) включает отправку аудио на транскрипцию и стриминг результатов[\[32\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L122-L130).
- **Диаризация/определение говорящих** - упоминается косвенно (выделено хранение говорящих в БД через speaker_id в модели Transcript[\[33\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L84-L91) и архитектура протокола diarize), хотя явно в шагах реализации фазы 2 не расписано. Судя по контексту, диаризация будет частью TranscriptService (получение результатов с пометкой говорящих).
- **Суммаризация встреч** - **важный момент**: план не содержит отдельного шага по суммаризации текста. Однако, в репозитории подготовлены компоненты для этого (есть proto summarize.proto и мок MockSummarizeClient[\[34\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/grpc_client.py#L40-L49)). Также в файле QUESTIONS.md приведена рекомендуемая модель для локальной суммаризации (Llama-3.1-8B-tldr)[\[35\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L74-L82) и на мермаид-схеме архитектуры видно, что после сохранения стенограммы в Postgres выполняется шаг **Summarizer**[\[36\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L88-L96)[\[37\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L90-L93). Вероятно, суммаризация подразумевается как часть общей обработки (например, после завершения расшифровки запускать summarizer). Тем не менее, в явном виде в фазах плана это не отражено - можно считать это упущением или запланированным расширением после реализации основных функций. Для полноты достижения цели проекта шаг интеграции суммаризации стоит добавить (например, как отдельный подпункт фазы 2 или дополнительную фазу).
- **Интерфейс голосового управления** - если подразумевалось именно управление системой голосом, это отдельный аспект (например, голосовые команды). Однако из исходного описания следует, что речь о распознавании контента встреч, а не управлении приложением голосом. В текущем плане функционал голосовых команд не фигурирует - и, судя по целям (стенография встреч), это не требуется. Таким образом, план соответствует цели стенографирования, а термин "голосовое управление" тут скорее относится к работе Codex с голосовыми данными, а не управлению интерфейсом.

**Корректность и достаточность плана:** План представляется **комплексным и логичным**. Он разбит на 4 фазы, каждая из которых закрывает необходимый пласт функционала, и дополняется разделом технического долга и будущих улучшений[\[38\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L189-L197)[\[31\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L198-L206). Такая структура позволяет поступательно достичь заявленной цели. Зафиксированные этапы (БД → обработка аудио → UI → авторизация) идут в правильном порядке: сначала обеспечивается ядро (данные и бэкенд-логика), затем пользовательский интерфейс и безопасность.

Отдельно отметим, что план учитывает **локальный офлайн-режим** (ставка на локальные модели Whisper, Silero VAD, pyannote, Llama и т.д.), что соответствует требованиям закрытого контура компании[\[29\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L156-L158). Также упомянуты важные детали, например, шифрование данных к 2025 году, что соответствует корпоративным политикам по хранению данных.

**Выявленные пробелы:** Кроме отсутствия явного шага по суммаризации, план не упоминает некоторые потенциально значимые аспекты, которые могли бы повлиять на архитектуру: - **Обработка потокового аудио vs файлов:** Вопрос, будет ли поддержка реального времени (например, запись встречи "на лету" с микрофона через WebRTC) или только загрузка готовых аудиофайлов. План и реализация сейчас ориентированы на загрузку файла встречи через /upload и последующую обработку. Если требуется **онлайн-распознавание по ходу встречи**, это потребует дополнительных решений (например, WebSocket/SSE поток с фронтенда на бэкенд). Судя по Questions.md, поднимался вопрос о правах на микрофон[\[39\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L146-L154) - возможно, планируется реализовать запись напрямую в браузере. Пока план и текущая реализация сосредоточены на офлайн-файле, что упрощает задачу на первых этапах. - **Масштабируемость и продакшн:** План отмечает будущее масштабирование (Kubernetes, множественные пользователи, внешние API)[\[31\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L198-L206), но эти пункты оставлены на долгосрочную перспективу. Это нормально для текущего этапа; просто важно держать в уме при разработке архитектуры (например, выбор технологий уже учитывает Kubernetes и CI/CD). - **UI/UX детали:** План технически перечисляет фронтенд-компоненты, но не описывает дизайн. Однако, ответ на вопрос о дизайне (использовать shadcn/ui компоненты) фактически дополняет план рекомендациями по UI-библиотеке[\[40\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L102-L110). Это можно считать корректировкой плана под предпочтения по интерфейсу.

В целом, **план можно признать полноценным и соответствующим цели** проекта. Все ключевые задачи обозначены. Необходимо лишь уточнить отдельные моменты (summarization, режимы работы с аудио) и скорректировать план при получении новой информации от заказчика (например, потребуется ли интеграция с SSO вместо локальной аутентификации, нужна ли поддержка записи с микрофона и т.д.). Эти вопросы частично перечислены агентом для уточнения (рассмотрим ниже).

## Анализ открытых вопросов (QUESTIONS.md)

В файле QUESTIONS.md ведется диалог между Codex-агентом и владельцем репозитория. На данный момент некоторые вопросы уже получили ответы, а ряд новых вопросов подготовлен для уточнения. Рассмотрим статус каждого и степень влияния на дальнейшую разработку:

- **Вопрос 1: PostgreSQL setup** - _«Предоставить экземпляр PostgreSQL или инструкции по подключению?»_ **Ответ получен**: владелец настроил установку Postgres скриптом, пароль 'password'[\[21\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L3-L10). **Блокирующий?** Нет. БД доступна локально, можно приступать к интеграции. Этот вопрос успешно решен, разблокировав Phase 1 (реализацию базы данных).
- **Вопрос 2: Frontend design assets** - _«Есть ли UI-макеты или компоненты (shadcn)?»_ **Ответ**: прямых макетов нет; рекомендовано использовать библиотеку **shadcn/ui**, подобрать цветовую палитру и логотип компании[\[41\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L122-L130). Владелец указал, что может предоставить бренд-гайд (цвета, логотип) позже[\[41\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L122-L130). **Блокирующий?** _Не блокирует основную разработку._ Использование стандартных компонентов shadcn/ui позволяет начать верстку уже сейчас с дефолтной темой. Отсутствие фирменных стилей - это скорее косметический нюанс: можно реализовывать UI, заложив возможность правки темы, а цвета/логотип добавить по мере их предоставления. Итого, вопрос требует уточнения (для придания окончательного вида UI), но не препятствует текущей работе над функциональностью фронтенда.
- **Вопрос 3: Model artifacts** - _«Локальные модели для ASR и speaker ID, их пути и интерфейсы?»_ **Ответ**: агент получил от модели **o3 pro** развёрнутый совет. Рекомендуется набор офлайн-моделей: Whisper Large-v3 (через faster-whisper), Silero VAD, pyannote diarization 3.1, NeMo Conformer для русского как опция, Llama-3.1-8B-tldr для суммаризации[\[42\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L34-L42)[\[43\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L44-L52). Владелец дополнительно уточнил параметры окружения:
- **GPU**: 1 GPU, 16 GB, CUDA 12.0[\[44\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L163-L170) - достаточный для вышеупомянутых моделей.
- **Хранение моделей**: на файловой системе GPU-сервера[\[45\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L164-L170) (12+ ГБ места нужно).
- **Длительность встречи**: ~2 часа макс[\[46\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L165-L170) (важно для планирования разбиения на фрагменты).
- **Политика хранения аудио**: пока можно хранить на диске, но в будущем предусмотреть защищенное хранилище[\[47\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L166-L170).

Таким образом, **вопрос закрыт** - определено, какие модели использовать и их объемы, а также ограничения окружения. **Блокирующий?** Нет. Теперь разработка может идти исходя из этих решений: можно интегрировать вызовы к **Whisper** и др. Присутствуют нюансы: - Нужны сами файлы моделей (возможно, их надо будет скачать и сохранить на сервере перед деплоем). - Объем моделей потребует настроить хранение (например, в папке models/ на сервере).

Но все это реализуемо без дополнительных ответов. Единственное - агент в ответе запросил от владельца подтверждения (GPU, хранение и пр.)[\[48\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L136-L144), и эти подтверждения получены, как указано выше. Дальнейшая работа (Phase 2) не блокируется.

- **Вопрос 4: Company-specific coding standards** - _«Есть ли особые стандарты кодирования?»_ **Ответ**: владелец ответил, что основные указания добавлены в AGENTS.md (backend и frontend style)[\[49\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L8-L11). Специфичных дополнительных стандартов нет, упор на то, что по фронтенду владелец полагается на решения разработчика. **Блокирующий?** Нет. Это означает, что текущие стандарты (Google-style docstrings, правила lint/mypy, структура React-компонентов и пр., которые уже описаны в репо) достаточны[\[50\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/AGENTS.md#L72-L80)[\[51\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/AGENTS.md#L84-L90). Агент может продолжать, придерживаясь этих соглашений.

После этих ответов агент (через o3 pro) сгенерировал ряд **новых вопросов для уточнения** (раздел "Что ещё попросить…" в QUESTIONS.md[\[39\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L146-L154)). Рассмотрим их и влияние:

- **Нужны ли схемы ролей в БД и LDAP/SSO?** - Спрашивается, требуется ли реализовать роли пользователей и интеграцию с корпоративным SSO (Single Sign-On) вместо локальной аутентификации. **Ответа пока нет** (владелец не отметил явным образом). **Блокирующий?** _Потенциально да, для задачи аутентификации._ Пояснение: Если корпорация требует SSO, реализация локального логина/пароля может пойти впустую или потребует переделки. План фазы 4 исходит из классической схемы с локальными пользователями и JWT[\[2\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L170-L178). Если же понадобится SSO, архитектура изменится (например, OAuth2/OpenID Connect провайдер вместо своей базы пользователей). Поэтому перед началом реализации аутентификации **важно уточнить** этот момент. Пока ответ не получен, наиболее разумно:
- Закладывать локальную auth (как в плане), но писать код модульно, чтобы при необходимости заменить провайдер (например, сделать отдельный модуль AuthService, который теоретически можно переключить на LDAP).
- Отложить полноценную разработку авторизации до получения ответа, либо начать с минимума (создание модели User и JWT-логики), понимая риски. В общем, вопрос _блокирует фазу 4_, но не влияет на более ранние фазы.
- **Список внутренних прокси/репозиториев для зависимостей** - Вопрос, нужно ли использовать корпоративные зеркала для pip/npm, либо особые правила скачивания (например, если прямой доступ к PyPI/NPM ограничен). **Ответа пока нет.** **Блокирующий?** _Не прямо сейчас._ Этот вопрос относится к окружению сборки/деплоя. На этапе разработки Codex уже устанавливает пакеты напрямую (судя по install_deps.sh, используется обычный pip) - значит, либо доступ в интернет есть, либо настроены прокси. Если в будущем CI/CD в корпоративной сети запрещает внешние подключения, нужно будет настроить репозитории. Пока разработка может продолжаться с использованием стандартных источников. Но перед деплоем в продакшн **требуется уточнить**: если есть ограничения, придется адаптировать сборочный процесс (настроить npm registry, pip index-url и т.д.). Итого, этот вопрос **не блокирует кодирование**, но важен для **инфраструктуры и CI**. Желательно получить ответ ближе к этапу внедрения CI/CD (в плане упомянуто внедрение GitHub Actions для тестов и линтеров[\[52\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L190-L197)).
- **Права на микрофон в браузере (Content-Security-Policy)** - Нужно ли учитывать корпоративную CSP, блокирующую getUserMedia (доступ к микрофону)? **Ответа нет.** **Блокирующий?** _Возможный блокер для функционала записи аудио с микрофона._ Если цель - только загрузка аудиофайлов, то этот вопрос не критичен. Но в ответах o3 pro были предложения реализовать веб-диктофон на фронтенде[\[53\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L104-L110). Если компания запрещает использование микрофона через браузер, функция записи напрямую в приложении не сможет работать без изменения CSP на уровне админов. Это значит:
- Либо ограничиться загрузкой готовых аудиофайлов (что уже предусмотрено).
- Либо узнать и попытаться повлиять на политику (что выходит за рамки разработки).

Пока вопрос открыт, **стоит фокусироваться на реализации загрузки файлов** (то, что точно работает) и предусмотреть архитектурно возможность добавить запись, но не тратить ресурсы на внедрение recorder сейчас. Этот нюанс не мешает выполнять текущий план (Phase 3 - UI загрузки/отображения транскрипции можно сделать без рекордера). Поэтому напрямую разработку **не блокирует**, но требует прояснения перед реализацией фичи записи звука.

- **Обновление моделей (частота, объем скачиваний)** - Нужно ли планировать регулярное обновление весов моделей и допустимый объем трафика. **Ответа нет.** **Блокирующий?** Нет. На функционирование текущей системы это не влияет. Скорее, это вопрос эксплуатации: можно ли периодически подтягивать новые версии моделей (если выходят обновления) или это нежелательно из-за нагрузки. Для разработки сейчас достаточно взять рекомендованные версии моделей и исходить из того, что они будут статичными в первой версии системы. В дальнейшем, зная политику, можно добавить механизмы обновления (или отказаться от них). В общем, это **уточнение не влияет на кодирование** в ближайшие спринты.
- **Локализация** - Нужно ли переводить UI на английский (для нерусскоговорящих). **Ответа нет.** **Блокирующий?** Нет. Разработчики уже встроили поддержку i18n (английский и русский есть)[\[54\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L23-L28). Вопрос скорее о приоритетах: если все пользователи - русскоговорящие, можно меньше внимания уделять англоязычному UI, но он и так практически готов из коробки (нужно лишь следить за переводом всех строк). Итак, это не мешает разработке. В худшем случае, если выяснится, что английский не нужен, можно будет удалить лишние строки, но сейчас двуязычность реализована, что покрывает любой вариант.

**Резюме по вопросам и блокерам:**

- **Блокирующие дальнейшую реализацию:** В настоящий момент критичных блокеров нет - благодаря ответам, агент имеет всю информацию для начала работ по Phase 1 и 2. _Однако_, перед выполнением Phase 4 (авторизация) **необходимо** получить решение по вопросу SSO (Вопрос 7.1). Это единственный вопрос, способный радикально поменять реализацию значимой части системы. Также желателен ответ по политике микрофона, чтобы правильно спланировать фронтенд (делать или нет встроенную запись звука).
- **Вопросы, требующие уточнения, но не блокирующие текущие задачи:** дизайн-материалы (брендинг), proxy для зависимостей, политика обновления моделей, языки интерфейса. Их прояснение улучшит качество результата и соответствие корпоративным требованиям, но разработку (особенно Phase 1-3) можно вести параллельно, закладывая возможность учесть эти детали позднее.

Ниже краткая маркировка открытых вопросов:

- **Q2. Дизайн/UI компоненты** - _Не блокирует._ Можно работать с shadcn/ui, брендинг уточнить позже.
- **Q3. Модели и артефакты** - _Решен._ Модели выбраны, ограничения учтены (блокировки нет).
- **Q7.1 SSO vs локальная аутентификация** - _Блокирует этап авторизации._ Нужен ответ до начала реализации auth.
- **Q7.2 Внутренние репозитории/прокси** - _Не блокирует код_, важно для деплоя (уточнить к моменту CI/CD).
- **Q7.3 Политика на микрофон (CSP)** - _Частично блокирует_ функцию записи с микрофона. Для текущей работы по файлам - не мешает, но требует ответа перед реализацией веб-диктофона.
- **Q7.4 Обновление моделей** - _Не блокирует._ Учитывается в архитектуре (весы хранятся локально), можно вернуться к этому при эксплуатации.
- **Q7.5 Локализация UI** - _Не блокирует._ Уже реализована базовая i18n, лишь уточнить необходимость английского.

## Рекомендации и следующие шаги

Исходя из анализа, предлагаем следующий **план действий**, скорректированный с учетом статуса разработки и уточнений. Задачи расставлены по приоритету, с указанием возможностей параллельной работы и необходимых предварительных условий:

### 1\. Реализация слоя данных (Phase 1, критический приоритет)

**Задача:** ввести полнофункциональную поддержку базы данных для хранения пользователей, встреч и расшифровок.

- **Модели и миграции:** Немедленно приступить к созданию SQLAlchemy-моделей User, Meeting, Transcript и соответствующих таблиц[\[1\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L36-L44)[\[55\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L84-L92). Использовать Alembic для генерации миграций и применить первую миграцию, создающую эти таблицы. _Зависимости:_ среда Postgres готова (есть скрипт установки, .env с DATABASE_URL). **Блокеров нет** - ответы по Postgres получены.
- **Сессия и репозитории:** Настроить подключение к БД через AsyncSession (откорректировать backend/app/db/session.py под реальный DATABASE_URL). Реализовать паттерн Repository: классы UserRepository, MeetingRepository, TranscriptRepository для абстракции CRUD операций[\[55\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L84-L92). _Пояснение:_ Это упростит тестирование и использование БД в бизнес-логике.
- **Интеграция с FastAPI:** Внедрить зависимости FastAPI (Depends) для сессии БД во всех нужных эндпоинтах. Обновить эндпоинты (например, /upload) чтобы сохранять данные о встрече в БД (метаданные: id, имя файла, время)[\[33\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L84-L91). Пока сам текст расшифровки заполняться не будет (его добавим на этапе обработки аудио).
- **Тесты:** Написать/обновить тесты для нового слоя данных[\[56\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L110-L118). Например, проверить, что при загрузке создается запись Meeting в БД, что репозитории корректно читают/пишут данные. Настроить тестовую БД (можно SQLite in-memory или отдельный PostgreSQL для тестов, указав TEST_DATABASE_URL). **Параллельность:** Все подзадачи Phase 1 тесно связаны с моделями, лучше выполнять последовательно одним участником. Однако, параллельно с этой задачей другой разработчик может начать работу над фронтендом (см. шаг 3), т.к. фронт не зависит прямо от наличия БД на первых порах (можно использовать заглушки API).

**Важно:** Перед началом Phase 4 (auth) - **уточнить вопрос SSO**. Если владелец ответит, что нужен LDAP/SSO, то на этом шаге нужно спроектировать модели/репозитории пользователей с учетом возможной интеграции (например, предусмотреть поле external_id, или хранить только минимальные данные пользователя). Если же будет подтвержден локальный сценарий, можно смело продолжать как запланировано.

### 2\. Интеграция реальной обработки аудио (Phase 2, высокий приоритет)

**Задача:** подключить локальные ML-модели для распознавания, идентификации говорящих (и суммаризации), заменив заглушки, чтобы система начала выдавать реальные результаты.

- **gRPC клиенты и протобуфы:** Сгенерировать Python-код из proto-файлов (transcribe.proto, diarize.proto, summarize.proto). Настроить реальных gRPC-клиентов для обращения к GPU-сервисам[\[32\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L122-L130). Возможно, для локальной реализации вместо gRPC будет вызываться локальная библиотека (например, faster-whisper), но архитектурно лучше оставить интерфейс как удаленный сервис (вдруг модели запускаются на отдельном GPU-сервере). Если GPU-сервисы планируются как отдельные микросервисы (по proto), то надо реализовать Channel с настройками TLS (используя GPUSettings с сертификатами[\[9\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/backend/app/core/settings.py#L34-L42)). _Параллельность:_ эту подзадачу можно начать, как только Phase 1 в работе. Генерация кода pb2 не зависит от БД. Разработчик А может заняться gRPC, пока разработчик B доделывает модели.
- **Реализация TranscriptService:** Заполнить логику метода stream_transcript(meeting_id). Здесь потребуется:
- Асинхронно отправлять аудио-файл (или аудиопоток) на сервис распознавания речи. Например, вызывать метод реального клиента (Whisper) и получать частичные результаты. Whisper (faster-whisper) поддерживает потоковую расшифровку фрагментами, это можно обернуть в AsyncGenerator, выдающий промежуточный текст.
- Параллельно или после получения полного текста, вызывать сервис диаризации (pyannote) на тот же аудиофайл или на получившуюся транскрипцию для отметки говорящих. Можно сначала получить готовую расшифровку целиком, затем диаризовать с временными метками - но тогда не будет онлайн-стриминга говорящих. Возможно лучше: получать от ASR промежуточные результаты с метками времени, буферизовать, и после завершения прогнать диаризацию для уточнения говорящих по сегментам.
- Сохранение результатов в базу: по мере готовности финального текста и спикеров - сохранить строки транскрипта (или помеченные сегменты) в таблицу Transcript, привязав к Meeting[\[27\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L46-L55)[\[57\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L48-L55). При стриминге, можно сначала отправлять на фронт промежуточный текст (без спикеров), а после пост-обработки диаризации - отправить обновление с метками говорящих.
- **Summarization:** учитывая цель проекта, стоит встроить и суммаризацию: после завершения расшифровки запускаем модель Llama-3.1-8B для генерации краткого итога встречи[\[19\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L86-L94)[\[37\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L90-L93). Результат сохраняем (например, поле Meeting.summary) и потенциально предлагаем через отдельный эндпоинт или включаем в детальный просмотр встречи. _Этот подпункт не был явно в плане, но раз пользователь ожидал такую функцию, лучше реализовать после расшифровки, как часть Phase 2 или отдельной небольшой Phase 2.5._ Для начала можно сделать суммаризацию синхронно (после получения всей транскрипции).
- Обработка ошибок: добавить обработку ситуаций, когда сервисы недоступны или падают, чтобы не зависал поток. При недоступности GPU-сервиса использовать fallback на mock (реализовано переключением GRPC_CLIENT_TYPE=mock)[\[58\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/grpc_client.py#L80-L88).

_Параллельность:_ TranscriptService зависит от наличия моделей и БД. Лучше приступать после того, как Phase 1 завершена или находится на финальной стадии (чтобы метод мог сразу писать в БД). Однако, часть работы (вызов Whisper/pyannote) можно прототипировать и без БД - используя временно запись результатов в память. Например, разработчик A может начать писать TranscriptService сразу после настройки gRPC клиентов, а разработчик B параллельно допишет сохранение в БД; затем объединить.

- **Интеграция и тесты:** Когда TranscriptService готов, нужно:
- Обновить эндпоинт /stream/{meeting_id} - сейчас он просто вызывает TranscriptService() и стримит, это останется, только теперь поток не пустой.
- Добавить интеграционные тесты: запуск мок-сервисов vs реальных. Например, пометить тесты, которые требуют реальные модели (можно сначала написать с mock, а для e2e в будущем - при наличии GPU). Проверить, что при передаче небольшого аудиофайла эндпоинт /stream отдает событие с текстом[\[59\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L140-L148).
- Проверить SSE: нет ли утечек, корректно ли завершается поток после конца расшифровки, поддерживается ли повторное подключение.

**Зависимости от вопросов:** Большинство вопросов для этого шага решены: модели выбраны, GPU характеристики известны (16 GB достаточно для Whisper Large-v2 в INT8). Важно только получить от владельца сами файлы моделей (или ссылки, если они у него имеются локально). Но это можно параллельно запросить/получить, пока идет разработка. Политика обновления моделей (вопрос 7.4) не мешает - достаточно скачать один раз.

### 3\. Разработка пользовательского интерфейса (Phase 3, средний приоритет)

**Задача:** создать базовый фронтенд для загрузки аудио и отображения результатов, используя компоненты shadcn/ui.

Эту работу можно выполнять **параллельно с Phase 1-2**, так как UI можно начинать до полной готовности бэкенда, имитируя ответы сервера. Разделим на подзадачи:

- **Компонент загрузки файла:** Реализовать страницу/виджет, позволяющий пользователю выбрать аудиофайл (drag & drop и/или кнопка выбора)[\[60\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L148-L156). Добавить отображение прогресса загрузки в реальном времени (пока можно фейковое, так как FastAPI UploadFile принимает файл целиком, но мы можем разбивать и показывать по таймеру). Валидацию типов файлов (разрешить только .wav, .mp3 и пр.)[\[60\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L148-L156). После выбора файла - вызывать API /upload и получать meeting_id. _UI/UX:_ Можно использовать готовые компоненты shadcn: &lt;Dialog&gt; для модального окна выбора файла, &lt;Progress&gt; для индикатора загрузки[\[53\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L104-L110). Для drag&drop может потребоваться кастомная логика, либо HTML5 + Tailwind стили.
- **Страница транскрипции (реалтайм):** После получения meeting_id, фронтенд должен подключиться к SSE-эндпоинту /stream/{meeting_id}[\[6\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L49-L54). Реализовать считывание событий SSE (через EventSource в JS) и обновление текста на экране. Отображать прогресс распознавания - например, показа последние фразы + индикатор, что процесс еще идет. **Спикеры:** По мере прихода данных, если в данных SSE сразу приходят идентификаторы говорящих (можем сделать формат data: {"speaker": "Speaker 1", "text": "фраза"}), UI должен группировать текст по спикерам, либо помечать разными цветами. Если пока приходят только строки текста, можно сначала отобразить плоско, а после завершения получить отдельно разбивку по спикерам из БД. Этот момент зависит от реализации TranscriptService. _Компоненты:_ можно применить Toast или простой список для live-обновлений текста[\[53\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L104-L110). Спикеров можно обозначать аватарками (Avatar) или цветными ярлыками.
- **Список встреч (Dashboard):** Создать страницу "Мои встречи" с таблицей или карточками встреч, доступных пользователю[\[61\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L68-L71). На этапе до авторизации - можно сделать список всех встреч в БД (пока пользователей нет, все не привязаны). Выводить id встречи, дату создания, возможно кнопки "Просмотр" (переход на страницу транскрипта) и "Экспорт". _Экспорт:_ можно реализовать простейший - кнопка "Download .txt" или .json, которая дергает новый эндпоинт /meetings/{id}/export (этот эндпоинт можно тоже сделать).
- **Шаблоны shadcn/ui:** Опираясь на рекомендации o3 pro[\[53\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L104-L110), можно воспользоваться готовыми примерами:
- Для таймлайна спикеров - шаблон компонента Timeline из awesome-shadcnUI.
- Для дашборда - шаблон Vercel AI voice-transcript (если есть ссылка, можно подсмотреть реализацию).
- Для SSE live updates - подсмотреть паттерн в Vercel AI Chatbot (они используют Toast или список сообщений обновляемый).
- Аудио-рекордер - упомянут Reddit demo. _Однако,_ до выяснения вопроса о CSP (микрофоне) этот подкомпонент можно отложить. Подготовить архитектурно место под него (например, кнопку "Record" в UI, пока не активную, или вариант: если getUserMedia доступен).
- **Тестирование UI:** Написать unit-тесты компонентов (Vitest + React Testing Library) - загрузки файла (мокаем функцию API, проверяем, что прогресс отображается, ошибки валидации показываются и т.д.)[\[62\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L152-L160), отображения транскрипции (мокаем EventSource, отправляем несколько событий - проверяем, что текст появилось и обновилось)[\[63\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L154-L162). Эти тесты обеспечить, что UI реагирует правильно на различные сценарии (нет соединения SSE, пустой текст, смена спикера и пр.).

**Параллельность:** Работа над фронтендом (Phase 3) может идти **параллельно** с Phase 1 и 2. Например: - Пока бекенд-разработчики заняты БД и сервисами, фронтенд-разработчик может уже сделать интерфейс загрузки и страницу транскрипции, используя заглушечные данные (например, имитировать SSE с таймером). - Это не конфликтует, а как только бэкенд реализует реальный SSE, фронтенд сразу готов его потреблять.

**Зависимости от вопросов:** Нет критических. Брендовые стили (вопрос 2) пока заменяются нейтральными. Английский язык (вопрос 7.5) - уже учтен, все тексты делать через i18n. Микрофон (вопрос 7.3) - функцию записи пока не делаем, ждем ответа; если станет известна возможность, добавим в будущем.

### 4\. Авторизация и привязка к пользователям (Phase 4, средний приоритет)

**Задача:** добавить систему пользователей, ограничить доступ к данным и обеспечить приватность.

- **Регистрация и вход:** Реализовать эндпоинты /auth/register и /auth/login (или использовать OAuth2 password flow). При регистрации - хешировать пароль (например, Bcrypt), сохранять пользователя в БД. При входе - проверять пароль, выдавать JWT. Использовать стандартные библиотеки (PyJWT или FastAPI OAuth2PasswordBearer) для выпуска и проверки токенов[\[2\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L170-L178). _Примечание:_ Если будет принято решение об SSO, то реализация изменится (напр. редирект на внешнюю страницу входа). На данный момент предполагаем локальный сценарий.
- **Middleware защиты:** Добавить в FastAPI dependency, которая проверяет Authorization заголовок JWT и загружает пользователя (например, get_current_user). Применить эту защиту к чувствительным эндпоинтам:
- /upload и /stream - вероятно, **должны требовать аутентификации**, иначе любой сможет загружать/слушать. Логично ограничить их авторизованными пользователями.
- Эндпоинты получения списка встреч, экспорта - тоже только для владельца.
- **Привязка встреч к пользователю:** При сохранении встречи в БД (в /upload) записывать user_id того, кто загрузил[\[64\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L176-L184). Тогда запрос списка встреч вернет только записи текущего пользователя. Нужно обновить соответствующие запросы в репозиториях (например, MeetingRepository.list_by_user(user_id)).
- **UI изменения:** Добавить на фронте формы регистрации/логина (можно минимально, или если SSO - тогда просто кнопку "Login via SSO"). Хранить токен (в localStorage или cookies), добавлять Authorization header к запросам. Отображать ошибки входа, и состояние сессии (например, "войти/выйти" кнопка).
- **Тестирование безопасности:** Добавить тесты, которые проверяют, что без токена эндпоинты возвращают 401, с неправильным токеном - 401, с правильным - доступ. Покрыть случаи, что пользователь A не может получить встречи пользователя B (это требует фабрикации нескольких пользователей и записей).

**Важное условие:** **Перед началом** реализации этих шагов нужно получить ответ на вопрос о **LDAP/SSO (Q7.1)**. Если заказчик подтвердит, что можно обойтись локальной auth, приступаем как выше. Если скажет, что нужен, например, OAuth2 на основе корпоративного провайдера, то план меняется: - Вместо собственных /register//login - настроить FastAPI OAuth2 AuthorizationCode flow (или другого), использовать pyAuthlib или Starlette встроенные средства, и не хранить пароли локально. - Это сильно повлияет на объем работы и время (возможно, проще делать после MVP локального, если разрешено).

Пока ответа нет, можно начинать с **поддержки локальной auth**, заложив расширяемость.

**Параллельность:** Авторизацию лучше делать **после** того, как Phase 1 (БД) завершена, потому что нужен User в БД, и **после/параллельно Phase 3**, чтобы интегрировать с фронтом. В принципе, разработчик C мог бы делать auth параллельно с тем, как разработчики A и B делают Phase 2 и 3. Нужно будет только синхронизоваться насчет моделей (User создается в Phase 1). То есть: - Если Phase 1 выполнена и модель User есть, то Phase 4 можно делать параллельно с Phase 2/3. - Frontend-часть (формы входа) тоже можно делать параллельно основной логике фронта, это мало связано с UI расшифровки.

### 5\. Тестирование, отладка и документация

После реализации основных фаз необходимо: - **Прогон всех тестов**, достижение успешного их выполнения. Удостовериться, что покрыты новые функции: БД, сервисы, UI. Добавить при необходимости новые тест-кейсы (например, интеграционные тесты SSE + auth: подключение к /stream без токена должно отказать, с токеном - работать). - **Документация:** обновить docs/ согласно сделанным изменениям. Например: - Описать новую схему БД и приложить ER-диаграмму[\[56\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L110-L118). - Обновить docs/process_overview.md или аналог - отразить реальный поток данных с указанием, где включаются модели. - Задокументировать REST API (список эндпоинтов, параметры) - можно сгенерировать из FastAPI документацию, а также вручную описать сценарии использования. - Добавить раздел про авторизацию (какой тип токена, как получать). - **Диаграммы:** при необходимости обновить sequence diagram процесса расшифровки, добавить диаграмму деплоя (если в будущем будет). - **Техдолг (CI/CD и прочее):** как отмечено в плане, в ближайшие спринты стоит настроить **GitHub Actions** для автоматического запуска линтеров (ruff, mypy) и тестов[\[52\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L190-L197). Эту задачу можно выполнять параллельно с разработкой функционала, поскольку она не влияет на код базы. Например, настроить workflow, который поднимает PostgreSQL сервис, запускает uv run pytest и npm test. Это может сделать отдельный участник команды.

### Параллелизация и зависимые задачи

В вышеприведенном плане подразумевается следующий **параллельный поток работ** (если ресурсы позволяют):

- _Backend Team:_
- Разработчик A - Phase 1 (модели, миграции, сессия). После завершения - Phase 2 (gRPC, TranscriptService).
- Разработчик B - Может подключиться к Phase 2 (например, написать интеграцию Whisper/STT, пока А делает миграции) или заняться тестами и документацией параллельно.
- Разработчик C - Phase 4 (auth) - начинается после того, как A сделает модель User. C может готовить auth-эндпоинты и JWT параллельно тому, как A/B отлаживают распознавание.
- _Frontend Team:_
- Разработчик D - Phase 3 UI (загрузка, SSE, список) - можно начинать сразу, пользуясь моками для API. D и C могут сотрудничать по части auth UI, когда C будет готов с бэкендом.
- Если D закончит UI раньше, он/она может затем помочь с тестированием фронта или подключиться к реализации более продвинутых фич (например, запись с микрофона, если решено внедрять).

**Последовательность vs ожидание ответов:**

Все предложенные шаги, кроме авторизации, можно выполнять **не дожидаясь дополнительных ответов**. В процессе разработки параллельно запрашиваем уточнения: - До этапа авторизации - получить ответ про **SSO**. - До добавления функциональности записи аудио - получить ответ про **CSP микрофона**. Однако, основной функционал загрузки файлов реализуется независимо.

Если к моменту начала Phase 4 ответ про SSO так и не будет получен, целесообразно реализовать локальную auth (чтобы не стопорить проект), но пометить в коде и документации, что возможен переход на внешний провайдер. Таким образом, владелец сможет протестировать систему с локальными аккаунтами, а при необходимости позже поручить заменить на SSO.

### 6\. Дальнейшие планы и отслеживание блокеров

После выполнения вышеописанных фаз проект приблизится к MVP, удовлетворяющему изначальные требования: локальная расшифровка встреч с отображением результатов и суммаризацией, в защищенном контуре.

Следует поддерживать тесную связь с владельцем по открытым вопросам. Рекомендуемые действия: - Регулярно обновлять QUESTIONS.md: после получения ответов помечать их как решенные и задавать новые возникающие вопросы. Например, уточнить требования к **экспорту транскрипций** (в какие форматы - PDF, DOCX? это упомянуто в техдолге[\[65\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L198-L201)) или к **аналитике**. - Следить за дедлайном по безопасному хранилищу (конец 2025) - начать проработку этого заранее, возможно вынести в отдельный план. - Планировать нагрузочное тестирование и оптимизацию после интеграции моделей, учитывая 2-часовые записи и 16 GB GPU (например, Whisper Large-v2 может обрабатывать ~4x real-time[\[43\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L44-L52), то есть 2 часа аудио ~ 30 минут обработки, что приемлемо).

В заключение, предлагаем придерживаться указанного плана, что согласуется с "Следующими шагами" из документа плана[\[66\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L210-L219), скорректировав его по необходимости после получения оставшихся ответов от владельца. Приоритизация и параллельное выполнение задач позволят эффективно продолжить разработку без простоев, а уточнение потенциальных блокеров (SSO, CSP и др.) на ранних этапах снизит риски больших переделок в будущем.

[\[1\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L36-L44) [\[2\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L170-L178) [\[3\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L7-L15) [\[11\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L14-L19) [\[14\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L18-L21) [\[15\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L23-L31) [\[22\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L42-L50) [\[23\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L16-L19) [\[24\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L52-L60) [\[25\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L74-L79) [\[26\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L16-L24) [\[27\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L46-L55) [\[30\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L140-L149) [\[31\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L198-L206) [\[32\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L122-L130) [\[33\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L84-L91) [\[38\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L189-L197) [\[52\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L190-L197) [\[54\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L23-L28) [\[55\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L84-L92) [\[56\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L110-L118) [\[57\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L48-L55) [\[59\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L140-L148) [\[60\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L148-L156) [\[61\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L68-L71) [\[62\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L152-L160) [\[63\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L154-L162) [\[64\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L176-L184) [\[65\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L198-L201) [\[66\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md#L210-L219) implementation_plan.md

<https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/docs/implementation_plan.md>

[\[4\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L24-L32) [\[5\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L34-L39) [\[6\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L49-L54) [\[7\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py#L56-L64) meeting.py

<https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/api/meeting.py>

[\[8\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/backend/app/main.py#L12-L16) main.py

<https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/backend/app/main.py>

[\[9\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/backend/app/core/settings.py#L34-L42) [\[10\]](https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/backend/app/core/settings.py#L43-L51) settings.py

<https://github.com/kvcop/Voicerec-By-Codex/blob/b9218493e8961824ada2b4a948c9bed110f3b26f/backend/app/core/settings.py>

[\[12\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/grpc_client.py#L14-L23) [\[13\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/grpc_client.py#L41-L49) [\[34\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/grpc_client.py#L40-L49) [\[58\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/grpc_client.py#L80-L88) grpc_client.py

<https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/backend/app/grpc_client.py>

[\[16\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L20-L23) [\[19\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L86-L94) [\[21\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L3-L10) [\[28\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L18-L26) [\[29\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L156-L158) [\[35\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L74-L82) [\[36\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L88-L96) [\[37\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L90-L93) [\[39\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L146-L154) [\[40\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L102-L110) [\[41\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L122-L130) [\[42\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L34-L42) [\[43\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L44-L52) [\[44\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L163-L170) [\[45\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L164-L170) [\[46\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L165-L170) [\[47\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L166-L170) [\[48\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L136-L144) [\[49\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L8-L11) [\[53\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md#L104-L110) QUESTIONS.md

<https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/QUESTIONS.md>

[\[17\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/README.md#L22-L30) README.md

<https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/README.md>

[\[18\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/docs/ai-context/project-structure.md#L128-L137) [\[20\]](https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/docs/ai-context/project-structure.md#L130-L137) project-structure.md

<https://github.com/kvcop/Voicerec-By-Codex/blob/d1975cc5ed25614460a935eb654cc180011e391c/docs/ai-context/project-structure.md>

[\[50\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/AGENTS.md#L72-L80) [\[51\]](https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/AGENTS.md#L84-L90) AGENTS.md

<https://github.com/kvcop/Voicerec-By-Codex/blob/cece92aaef8278d7b211b0073a4fc213e1aa30e2/AGENTS.md>