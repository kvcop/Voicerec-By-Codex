# TODO

Отмечайте прогресс в колонке **DONE**: пока задача не выполнена, ставьте «✗», после завершения замените на «✓».

## Part 3 2025-09-28

Зависимости - нет, отдельные сервисы GPU.

Ниже перечислены задачи для создания и настройки трех GPU-сервисов: ASR, Diarization, Summarization. Формат аналогичен существующему TODO: каждая задача имеет ID, статус, описание, область, приоритет и зависимости.

**Важно:** На момент начала работ папка для GPU-сервисов пуста - нужно создать структуру. Предлагается завести каталог gpu_services/ в корне, где будут подпапки или файлы для каждого сервиса, а также общий код (например, утилиты, модели, Dockerfile).

Приоритеты: P0 - критически важно для запуска основной функциональности, P1 - важные улучшения, P2 - дополнительные (не блокирующие).

| ID       | DONE | Task (Описание задачи)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Area                     | Priority | Dependencies                         |
| -------- | ---- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------ | -------- | ------------------------------------ |
| **G1**   | ✗    | **Сервис распознавания речи (ASR) – Whisper Large-v2** – *Разработка разбита на подзадачи G1_x ниже*. Создать отдельный сервис для ASR с использованием модели Whisper.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | GPU/ASR                  | P0       | — (начало работ)                     |
| **G1_1** | ✓    | **Подготовка окружения для ASR** – Установить необходимые зависимости для сервиса распознавания: PyTorch (с поддержкой CUDA), torchaudio, transformers, и пакет Whisper. Убедиться, что модель **Whisper Large-v2** доступна (скачивается через HuggingFace или `openai-whisper`). Настроить конфигурацию: в `.env` или настройках добавить параметр (например, `ASR_MODEL_SIZE`), чтобы можно было указать меньшую модель (например, `medium` или `small`) для тестирования на CPU.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | GPU/ASR                  | P0       | (см. список зависимостей выше)       |
| **G1_2** | ✓    | **Структура кода для ASR-сервиса** – Создать каталог `gpu_services/` в корне репозитория (если еще не создан). В нем создать модуль `gpu_services/asr_service.py`. Сгенерировать (или убедиться в наличии) gRPC-код из `transcribe.proto` – файлы `transcribe_pb2.py` и `transcribe_pb2_grpc.py` (они уже сгенерированы в `backend/app/clients/`). В `asr_service.py` реализовать запуск gRPC-сервера: импортировать классы из сгенерированных файлов (`TranscribeServicer`, функцию регистрации сервера). Создать класс `ASRService(TranscribeServicer)` и определить метод `Run(self, request, context)`. В функции `main` (если запускается как скрипт) создать `grpc.Server`, зарегистрировать `ASRService` через `add_TranscribeServicer_to_server`, слушать порт (из переменных окружения, напр. `50051`).                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | GPU/ASR                  | P0       | protos сгенерированы                 |
| **G1_3** | ✓    | **Загрузка модели Whisper в сервисе** – В конструкторе или при старте `ASRService` загрузить модель **Whisper Large-v2** с помощью HuggingFace Transformers. Использовать половинную точность: `model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2", torch_dtype=torch.float16).to("cuda")` (если GPU доступен). ПредусмотретьFallback: если GPU недоступен (например, в среде тестирования), загрузить модель на CPU (и/или загрузить более маленькую модель, исходя из параметра `ASR_MODEL_SIZE`). Убедиться, что загрузка происходит один раз при запуске сервиса.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | GPU/ASR                  | P0       | G1_1 (библиотеки установлены)        |
| **G1_4** | ✓    | **Реализация метода Run для ASR** – Реализовать логику в `ASRService.Run(request, context)`: открыть аудиофайл по пути `request.path` (файл будет лежать в общей папке, например, `data/raw/<filename>.wav`, доступной и сервису). При необходимости конвертировать аудио к требуемому формату (16 kHz, mono) – можно использовать `torchaudio.load` и, если нужно, `torchaudio.functional.resample` или вызывать ffmpeg. Пропустить аудио через модель Whisper: получить либо полный распознанный текст, либо список сегментов с таймстемпами (например, используя `model.transcribe` из openai-whisper или `pipeline` с `return_timestamps=True`). Если модель выдала сегменты, включить их в результат. **Важно:** после получения текста выполнить пост-обработку – заменить в полученном тексте известные ошибки (например, regex замена `\bрум\b` на **RUMA** и другие оговоренные термины). Подготовить объект ответа `Transcript` (или аналогичный, в протоколе пока только поле `text`) – поместить либо весь текст, либо при необходимости сериализовать сегменты в строку (временно, если протобуф не поддерживает список сегментов). Вернуть результат через gRPC. Добавить логирование времени выполнения распознавания (например, замерять время до/после `model` inference). | GPU/ASR                  | P0       | G1_3 (модель загружена)              |
| **G1_5** | ✓    | **Тестирование сервиса ASR** – Протестировать работу `ASRService`. Локально (без GPU) запустить сервис (он должен уметь работать на CPU с небольшой моделью). Воспользоваться короткой аудиозаписью (несколько секунд речи) для проверки. Написать простой клиент или использовать `grpcurl`/unit-тест, чтобы отправить `AudioRequest(path="...")` и получить распознанный текст. Убедиться, что сервис возвращает осмысленный результат, а пост-обработка заменила специфические слова правильно. При необходимости, откорректировать код.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Tests                    | P0       | G1_4 (сервис реализован)             |
| **G2**   | ✗    | **Сервис диаризации (Speaker Diarization)** – *Разработка разбита на подзадачи G2_x ниже*. Создать отдельный gRPC-сервис для диаризации речи (определение говорящих по аудио).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | GPU/Diarization          | P0       | protos, ASR output format            |
| **G2_1** | ✓    | **Подключение модели диаризации** – Выбрать и подготовить инструмент для диаризации. Базовый вариант – **NVIDIA NeMo** toolkit. Убедиться, что NeMo установлена (`nemo_toolkit[asr]`), а также зависимости (например, `onnxruntime` для VAD, `omegaconf`). Альтернатива – **pyannote.audio** (установить при необходимости, потребуются pre-trained модели и токен HF). На этом шаге следует скачать/разместить необходимые модели: для NeMo – конфигурацию diarization (например, готовый YAML `diar_inference.yaml`) и связанные чекпойнты (VAD, speaker embedding, MSDD) либо использовать встроенный Pipeline. Проверить, что эти ресурсы доступны (например, положить в `gpu_services/models/` или загружать при запуске).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | GPU/Diarization          | P0       | G1_1 (torch, Nemo готовы)            |
| **G2_2** | ✓    | **Структура кода для сервиса диаризации** – Создать модуль `gpu_services/diarize_service.py`. Сгенерировать/проверить gRPC код из `diarize.proto` (файлы `diarize_pb2.py`, `diarize_pb2_grpc.py` есть в проекте). Реализовать класс `DiarizeService(DiarizeServicer)` с методом `Run(AudioRequest)`. В функции запуска (main) поднять `grpc.Server` на отдельном порту (например, 50052) и зарегистрировать `DiarizeService` через `add_DiarizeServicer_to_server`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | GPU/Diarization          | P0       | protos сгенерированы                 |
| **G2_3** | ✓    | **Реализация диаризации в Run** – В методе `Run` сервиса диаризации реализовать полный цикл: получить `request.path` аудиофайла, загрузить аудио (при необходимости привести к нужному формату, как и в ASR). Выполнить диаризацию с помощью выбранной модели. Для NeMo: настроить `NeuralDiarizer` через YAML-конфиг – загрузить конфиг (OmegaConf), указать путь к аудио, пути к моделям, и вызвать diarizer.model.diarize() либо воспользоваться готовым методом (в новых версиях может быть `ClusteringDiarizer` класс). Для pyannote: загрузить pre-trained пайплайн, вызвать `pipeline(audio_path)`. Результат диаризации – список сегментов с метками говорящих и временами. Сформировать ответ `DiarizationResult`: заполнить список `segments` (поля start, end, speaker). Присвоить **метки говорящих**: например, "Speaker 1", "Speaker 2", … в порядке их первого появления в аудио (если NeMo/pyannote не присваивает автоматически). Если в аудио один говорящий, вернуть один сегмент покрывающий все аудио (Speaker 1). Учесть, что диаризация может работать долго на длинных файлах – при необходимости, добавить логирование прогресса (например, вывести сообщение после завершения VAD или каждого этапа).                                                             | GPU/Diarization          | P0       | G2_1 (модели готовы)                 |
| **G2_4** | ✓    | **Тестирование сервиса диаризации** – Проверить работу `DiarizeService` на небольшом аудио. Например, взять отрывок разговора с 2 спикерами длиной ~30 секунд. Запустить сервис локально (можно на CPU, хотя медленно – зато короткий файл). Отправить gRPC-запрос `AudioRequest` и дождаться `DiarizationResult`. Убедиться, что возвращается список сегментов с разными `speaker` (как минимум "Speaker 1", "Speaker 2") и корректными временными метками порядка величины длительности файла. При отсутствии доступной GPU для теста, можно временно подключить более простую модель (например, pyannote с небольшими моделями) или сократить файл до 10 сек. При успешном тесте – диаризация работает.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Tests                    | P0       | G2_3 (сервис реализован)             |
| **G3**   | ✗    | **Сервис суммаризации (LLM)** – *Разработка разбита на подзадачи G3_x ниже*. Создать сервис для генерации резюме встречи на основе полного текста транскрипта.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | GPU/Summarization        | P0       | protos, ASR текст готов              |
| **G3_1** | ✓    | **Структура кода для сервиса суммаризации** – Создать модуль `gpu_services/summarize_service.py`. Убедиться в наличии сгенерированных `summarize_pb2.py` и `summarize_pb2_grpc.py`. Реализовать класс `SummarizeService(SummarizeServicer)` с методом `Run(TextRequest)`. В main-функции запустить gRPC-сервер на порту (например, 50053) и зарегистрировать сервicer через `add_SummarizeServicer_to_server`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | GPU/Summarization        | P0       | protos сгенерированы                 |
| **G3_2** | ✓    | **Реализация вызова LLM в Run** – В методе `Run(self, request, context)` получить `request.text` – полный транскрипт встречи. Реализовать обращение к внешней LLM (в компании имеется развёрнутая модель, совместимая с OpenAI API, например **Qwen-3**). Получить URL/endpoint и API-ключ из конфигурации (переменные окружения `LLM_API_BASE`, `LLM_API_KEY`, добавить их в `.env` и `GPUSettings`). В простейшем случае отправлять весь текст одним запросом (через библиотеку `openai` или HTTP POST). Задать параметры: модель (если требуется), **низкую температуру** (около 0.2-0.3 для консистентности результата). Получить от API сгенерированный summary.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | GPU/Summarization        | P0       | G1_1 (библиотека openai установлена) |
| **G3_3** | ✓    | **Обработка длинного текста (chunking)** – Если транскрипт очень большой (например, более ~5000 слов или близок к ограничению контекста ~4096 токенов), реализовать логику разбиения: разбить текст на несколько частей (например, ~N символов или по абзацам, чтобы каждая часть помещалась в контекст). В цикле вызвать LLM API для каждой части с промптом вроде *«Суммируй следующий фрагмент: ...»*. Полученные частичные summary соединить. Если частей было несколько, затем отправить объединенный итог (или весь исходный текст + частичные резюме) ещё раз в LLM для финального суммирования всего контента. Этот многоступенчатый процесс описать в коде с комментариями, чтобы было понятно, как он работает.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | GPU/Summarization        | P0       | G3_2 (базовый вызов готов)           |
| **G3_4** | ✓    | **Обработка ошибок и кодировка** – Добавить обработку ошибок при вызове LLM API: обернуть запросы в try/except, при исключениях или HTTP 5xx ответах логировать ошибку и возвращать gRPC-статус **UNAVAILABLE** или соответствующий код. Убедиться, что текст на русском не теряет кодировку (при использовании библиотеки openai это обычно автоматически, но если через requests – правильно указать `.encode('utf-8')` при необходимости). После получения итогового резюме сформировать объект ответа `Summary` (с полем text) и вернуть через gRPC.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | GPU/Summarization        | P0       | G3_3 (логика суммирования готова)    |
| **G3_5** | ✓    | **Тестирование сервиса суммаризации** – Проверить `SummarizeService`: для теста можно не гонять реальную большую LLM-модель, а использовать небольшой текст. Запустить сервис (можно без GPU, так как он не использует локальных моделей). Вызвать метод `Run` с небольшим `TextRequest.text` (например, пару абзацев осмысленного текста) и убедиться, что приходит осмысленный ответ (если доступен реальный API). Если API недоступен в тестовой среде, можно замокать вызов LLM (например, временно возвращать фиксированный текст в Run) – важно проверить, что gRPC-взаимодействие работает.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Tests                    | P0       | G3_4 (сервис реализован)             |
| **G4**   | ✗    | **Docker-окружение для GPU-сервисов** – *Разбито на G4_x ниже*. Подготовить контейнеры и оркестрацию для всех новых сервисов (ASR, Diarization, Summarization).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | DevOps/Container         | P0       | G1, G2, G3 (код готов)               |
| **G4_1** | ✗    | **Написание Dockerfile** – Создать Dockerfile (например, `gpu_services/Dockerfile`) для сборки образа с GPU-сервисами. Использовать базовый образ с **CUDA** и Python (например, `nvidia/cuda:11.8.0-runtime-ubuntu20.04` с установкой Python 3.10, либо образ `nvidia/cuda:12.2.0-runtime-ubuntu20.04` – тогда убедиться, что ставится совместимая версия PyTorch). В Dockerfile установить все требуемые зависимости: командой `pip install` установить `torch`, `torchvision`, `torchaudio` (с нужной CUDA, через `--extra-index-url https://download.pytorch.org/whl/cu118`), `transformers==<версия>`, `git+https://github.com/openai/whisper.git` (или `openai-whisper`), `nemo_toolkit[asr]`, `openai` (клиент для API) или `requests`, а также `protobuf`, `grpcio`, `grpcio-tools`. При необходимости установить системные пакеты: например, `ffmpeg` (через apt) для обработки аудио. Убедиться, что версии совпадают с требованиями (PyTorch и CUDA).                                                                                                                                                                                                                                                                                                                            | DevOps/Container         | P0       | G1_1 (список зависимостей)           |
| **G4_2** | ✗    | **Конфигурация docker-compose** – Обновить файл `infra/docker-compose.gpu.yml` для запуска всех трех сервисов. Для каждого сервиса (asr, speaker, summarizer) добавить сервис в YAML: использовать образ, собранный Dockerfile-ом G4_1. В `command` или `entrypoint` указывать запуск соответствующего сервиса, например: `python -m gpu_services.asr_service` для ASR, и аналогично для других. Назначить каждому сервису порт (например, asr – 50051, diarization – 50052, summary – 50053) и пробросить их наружу (`ports:` в compose). Смонтировать общий том с аудиофайлами: например, монтировать локальную папку `./data/raw` в контейнеры на тот же путь (чтобы сервисы находили файлы по одинаковому пути). Если планируется TLS, продумать монтаж сертификатов (например, папку `certs/` в контейнер). Для доступа к GPU в Docker Compose добавить для каждого сервиса параметр `deploy.resources.reservations.devices` с capability `gpu` (или запустить с флагом `--gpus all`).                                                                                                                                                                                                                                                                                                 | DevOps/Container         | P0       | G4_1 (образ готов)                   |
| **G4_3** | ✗    | **Тестирование контейнеров** – Запустить локально `docker-compose -f infra/docker-compose.gpu.yml up` и убедиться, что все три контейнера стартуют. Проверить лог вывода: ASR сервис должен сообщить о загрузке модели Whisper, Diarization – о загрузке моделей диаризации, Summarization – о запуске и ожидании запросов. Затем запустить сквозной тест: поднятый FastAPI-бекенд (в режиме grpc-клиентов) и загруженный через API аудиофайл. Убедиться, что бекенд успешно соединяется с gRPC-сервисами внутри compose и получает результаты. Если какие-то переменные (например, HOST) настроены неправильно – скорректировать Compose или .env (в docker-compose можно задать `GPU_GRPC_HOST` = имя сервиса контейнера, например, `asr`). После успешного запуска – контейнеризация готова.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | DevOps/Container         | P0       | G4_2 (compose настроен), I3          |
| **G4_4** | ✗    | **Документация запуска GPU-сервисов** – Обновить README (или docs) проекта: описать, как запустить GPU-версии сервисов. Добавить инструкцию про `docker-compose.gpu.yml`: например, требование установить NVIDIA Docker runtime, команду запуска, переменные окружения (`GPU_GRPC_HOST`, `GPU_GRPC_PORT` или отдельные порты). Отметить, что в локальной отладке без GPU можно использовать небольшие модели или мок-сервисы. Также указать, что в продакшне возможно разнести сервисы на разные GPU/машины (и тогда нужно настроить переменные HOST/PORT соответствующим образом).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Docs                     | P0       | G4_3 (все запущено локально)         |
| **G5**   | ✗    | **Тонкая настройка точности ASR (доменные слова)** – Проанализировать качество распознавания на доменном словаре (например, название **RUMA** и другие специфичные термины). На этом этапе, после запуска базового сервиса, провести тесты на нескольких известных записях. Если замечены искажения (как "рум" вместо "RUMA"), дополнить пост-обработку. Составить список проблемных слов и их правильных форм. Реализовать дополнительную логику замены/нормализации: например, помимо простого `RUMA` возможно добавить другие термины, аббревиатуры, которые часто искажаются. Изучить возможность **Whisper prompt**: подача правильных терминов в качестве подсказки модели (если API Whisper или HF Model позволяет). Если улучшения незначительны, оставить простую замену. Задача считается выполненной, когда на тестовых записях ключевые термины распознаются правильно. Описать внесенные изменения (в коде комментариями или отдельной заметкой в документации).                                                                                                                                                                                                                                                                                                               | GPU/ASR Quality          | P1       | G1 (базовый ASR работает)            |
| **G6**   | ✗    | **Сравнение Pyannote vs NeMo (опционально)** – *Исследовательская задача.* Попробовать альтернативный подход к диаризации с библиотекой **pyannote.audio** и сравнить с текущей (NeMo) по качеству и скорости. Установить `pyannote.audio` и необходимые модели (например, `pyannote/speaker-diarization@2023.07` через HuggingFace, потребует токен). Взять те же тестовые аудио, прогнать диаризацию через pyannote Pipeline. Сравнить результаты сегментации и метки: насколько отличаются от NeMo, есть ли ошибки (слияние говорящих, пропуски). Замерить время выполнения и потребление памяти для каждого подхода. Сделать вывод: если pyannote дает сопоставимое качество и проще в использовании, можно в будущем переключиться на него (например, для среды без мощных GPU). Результаты эксперимента оформить в документе (например, дополнить `docs/researches/speech_stack_research.md` или создать новый отчет) с выводами. **Замечение:** эта задача не блокирует основной функционал (низкий приоритет, P3).                                                                                                                                                                                                                                                                  | GPU/Diarization Research | P3       | G2 (основная диаризация готова)      |
| **G7**   | ✗    | **Единый GPU-сервис (режим all-in-one)** – *Опционально, для удобства разработки.* Реализовать возможность запуска всех функций обработки в одном процессе (на одном gRPC сервере). Создать модуль `gpu_services/unified_server.py`, где импортировать `ASRService`, `DiarizeService`, `SummarizeService`. Запустить один `grpc.Server` и зарегистрировать все три servicer на разных gRPC-классах (т.е. один процесс слушает, например, порт 50051 и обрабатывает три типа запросов). Добавить флаг в настройки, напр. `GPU_SINGLE_PROCESS=true`, чтобы переключаться на этот режим (в FastAPI можно тогда использовать один хост:порт). Этот режим удобен для локального тестирования, если на одной машине/GPU память позволяет загрузить сразу Whisper и NeMo. Проверить потребление памяти: Whisper Large (FP16) ~5-6GB, NeMo модели ~2-3GB, итого ~8GB – поместится на 16GB GPU. Если нет, документировать, что единый режим только для мощных карт. Протестировать unified-сервис локально: должен работать как три в одном. Оставить в документации примечание, что в production лучше разделенные сервисы, а unified – для отладки.                                                                                                                                                | GPU/All-in-One           | P2       | G1, G2, G3 (сервисы сделаны)         |


## Part 1 2025-09-26
Part 1 is DONE and is located in `docs/TODOS/part_001_initial.md`.

## Part 2 2025-09-27
Part 2 is IN_PROGRESS and is located in `TODO.md`

## Part 4 2025-09-27
Part 4 is POSTPONED, is waiting for Parts 2 and 3 and is located in `TODO_INTEGRATION.md`

После выполнения ВСЕХ TODO в файле, файл целиком перемещается в `docs/TODOS/part_XXX_{phase}.md`.
