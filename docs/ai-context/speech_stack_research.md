# Исследование стеков распознавания речи

Этот документ консолидирует ответы владельца репозитория и результаты запроса к модели "o3 pro" по выбору локальных моделей для сервиса стенографирования встреч. Информация перенесена из `QUESTIONS.md`, чтобы сохранить её после очистки файла от закрытых вопросов.

## Рекомендованные модели распознавания речи

| Модель | Языки | Качество на русском | Скорость | Память | Лицензия | Примечание |
| ------ | ------ | ------------------- | -------- | ------ | -------- | ---------- |
| Whisper Large-v3 (faster-whisper, 1.6B) | 98+ | ~5% WER на чистом аудио | ≈4× real-time на RTX 3080 | 3.3 ГБ INT8 | MIT | Основная модель по умолчанию |
| NeMo STT RU Conformer-CTC-Large | ru | 3.5% на RuLS | ≈6× real-time | 0.5 ГБ FP16 | BSD-3 | Использовать как fallback для чисто русского канала или слабых GPU |
| FastConformer Streaming (NeMo) | 70+ | WER ниже Whisper v3 на FLEURS ru | <200 мс latency | 0.6 ГБ | BSD-3 | Для потокового режима |
| Silero-STT | ru, en | 6% (ru clean) | ≈8× real-time на CPU | 120 МБ | Apache-2 | CPU-only альтернатива |

**Совет:** хранить две сборки: `whisper-large-v3-int8.bin` (основная) и `stt_ru_conformer_large.nemo` (fallback). В тестах модели мокируются адаптерами, возвращающими подготовленные ответы.

### Ускорение и квантование
- Использовать CTranslate2 для 8-битного INT8 и ускорения Whisper до ~4×.
- Изучить патчи Fireworks.ai для дополнительного ускорения (открыты в публичном доступе).

### Voice Activity Detection
- Применять Silero VAD (`silero_vad.forward(wave_chunk)`) — <1 мс на 30 мс фрейм, лучшая точность по сравнению с WebRTC VAD.

## Диаризация и идентификация говорящих
1. `pyannote/speaker-diarization 3.1` — PyTorch, DER ≈7% на AMI IHM.
2. NVIDIA NeMo speaker diarization (ECAPA-TDNN embeddings + clustering) — более высокая точность для мультиголоса.
3. Сопоставление повторяющихся голосов выполнять по cosine similarity эмбеддингов, порог ≈0.7; пользовательская разметка хранится в таблице `speaker_profile(id, embedding)`.

## Локальная суммаризация встреч

| Модель | Объём контекста | Качество (AMI) | Память | Примечание |
| ------ | ---------------- | -------------- | ------ | ---------- |
| Llama-3.1-8B-tldr | 8k токенов | ROUGE-1 46.3 / ROUGE-2 12.7 F1 | ≈8 ГБ INT8 | Базовая модель для длинных стенограмм |
| Mixtral-8×7B-Instruct-Summ | 32k токенов | ROUGE-1 48.1 / ROUGE-2 13.0 | ≈14 ГБ INT8 | Лучшая когерентность при аудио >30 мин |
| HERA post-process | n/a | +4 ROUGE-L | любое | Предварительная сегментация для длинных текстов |

**Пайплайн:** STT → объединение по говорящим → сегментация ≤8k токенов → запуск summarizer. Для входов >8k токенов применять HERA segmentation.

## Инфраструктура и безопасность GPU-сервисов
- Предпочитать gRPC (стриминг из коробки, минимальный overhead).
- Контейнеризация: три Docker-образа `asr`, `speaker`, `summary`, каждый запускается с `--gpus "device=0"`.
- Разрешено использовать собственные Python entry-point вместо Triton Inference Server для упрощения моков.
- Для защищённого канала между backend и GPU-сервисами требуется VPN или mTLS (подробности в `docs/gpu_security.md`).

## Стратегия обработки двухчасовой записи

| Этап | Настройки | Пояснение |
| ---- | --------- | --------- |
| VAD | окно 0.5–1 с | Отсечение тишины, экономия ~30% времени |
| ASR | блоки 30–45 с с overlap 5 с | Баланс между скоростью и контекстом; overlap убирается постпроцессингом |
| Диаризация | сегменты 3–5 мин | Сначала грубая диаризация, затем уточнение внутри блоков ASR |
| Суммаризация | окна 6000 токенов | 2 ч ≈ 50k токенов → 9 окон Llama-3-8B |

## Ответы владельца на дополнительные вопросы
- mTLS между backend и GPU-сервисами обязателен.
- Одновременно обрабатывается одна встреча; сервисы могут работать синхронно.
- Достаточно пост-обработки встреч (live-caption не требуется).
- Формат выгрузки summary пока не задан.
- Срок переноса аудио в защищённое хранилище — до конца 2025 года (указано также в `README.md`).

## Источник
Информация получена от владельца репозитория через `QUESTIONS.md` и перенесена сюда  для сохранения контекста.
