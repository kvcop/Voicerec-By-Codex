date: 2025-09-27

# Обработка аудио: модели, сервисы GPU и интеграция

## Введение и потребности сервиса

Для системы распознавания встреч требуются три основных компонента обработки аудио:

- **Распознавание речи (ASR)** - преобразование аудиозаписи в текст расшифровки.
- **Диаризация (Speaker Diarization)** - определение, кто и когда говорит, то есть разметка текста по говорящим.
- **Суммаризация** - генерация краткого резюме встречи на основе полной расшифровки.

Аудитория преимущественно русскоязычная, поэтому модели должны хорошо работать с русским языком. Качество распознавания и суммаризации является приоритетом, в то время как эффективность (скорость) важна, но не критична - можно обрабатывать запросы последовательно, ставя их в очередь при необходимости. Ниже рассмотрены лучшие на текущий момент подходы для каждого компонента (кроме LLM, т.к. готовая LLM-модель для суммаризации уже доступна) и описаны планы по реализации GPU-сервисов, отвечающих за каждую задачу.

## Распознавание речи (ASR) - выбор модели и загрузка

**State of the Art (русский язык):** Самой точной открытой моделью распознавания речи, поддерживающей русский язык, является **OpenAI Whisper**, особенно ее крупная версия _Whisper Large-v2_. Whisper обучена на **680 тыс. часов мультиязычных данных (96 языков)** и демонстрирует передовое качество расшифровки на многих языках[\[1\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=OpenAI%20Whisper%3A%20Seq2Seq%20for%20A,utomatic%20Speech%20Recognition). Для русского языка Whisper показывает очень высокую точность по сравнению с предыдущими моделями (например, Vosk, Silero и др.), поэтому **Whisper Large** - приоритетный выбор для нашего сервиса распознавания. Альтернативы вроде моделей Silero STT (от компании Silero) могут работать быстрее на CPU для русского, но они существенно уступают Whisper в точности. Мы ориентируемся на максимальное качество, используя GPU, поэтому Whisper Large подходит лучше всего.

**Модель Whisper и дообучение:** Whisper - уже хорошо обученная модель, и специального дообучения под русский язык обычно не требуется. Однако могут возникать проблемы с **именами собственными** и специфическими терминами. Пользователь упоминал, что название продукта «RUMA» иногда распознается как «рум». У Whisper есть механизм подсказки контекста (prompt) - можно подать модели начальную фразу с правильным написанием терминов, чтобы повысить шанс корректного распознавания[\[2\]](https://cookbook.openai.com/examples/whisper_prompting_guide#:~:text=Whisper%20may%20incorrectly%20transcribe%20uncommon,can%20use%20prompts%20to). Однако более практичный путь - **постобработка**: после получения текста выполнять поиск и замену типичных ошибок (например, регекс \\bрум\\b → RUMA). В будущем возможно рассмотреть легкое дообучение (fine-tuning) модели на небольшом наборе данных, содержащих проблемные слова, либо внедрение словаря для biasing (некоторые энтузиасты предлагали подходы к "дополнению словаря" Whisper[\[3\]](https://github.com/openai/whisper/discussions/1268#:~:text=Introducing%20helping%20dictionnary%20with%20acronyms%2C,decide%20properly%20when%20in%20doubt)). На начальном этапе мы реализуем простую пост-правку расшифровки для исправления «RUMA» и других известных искажений.

**Загрузка модели Whisper:** Модель Whisper Large довольно объемная (~1.5 млрд параметров, ~3ГБ на диске, ~10ГБ VRAM в FP32). Для эффективного использования GPU: - Загрузить модель в половинной точности (FP16) - фреймворк transformers позволяет это сделать (параметр torch_dtype=torch.float16 при вызове from_pretrained). В FP16 модель займет ~5-6 ГБ видеопамяти, что вписывается в возможности современных GPU 10-16 ГБ. - Заранее загрузить модель **один раз при старте сервиса**, а не при каждом запросе. Инициализация модели - дорогая операция, поэтому гRPC-сервис должен держать модель в памяти. - Использовать ускорения: устройство CUDA, опция device_map="auto" или прямое .to("cuda") для отправки модели на GPU. Убедиться, что установлены необходимые зависимости (PyTorch >= 2.x, transformers и openai-whisper или whisper пакет). - Входные данные: перед распознаванием нужно прочитать WAV-файл и, при необходимости, привести к нужному формату (Whisper ожидает 16 kHz mono WAV). Если файл не в требуемом формате, можно воспользоваться ffmpeg или библиотекой librosa/torchaudio для конвертации. - Whisper сам сегментирует длинное аудио на куски ~30 секунд и обрабатывает их последовательно[\[4\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Whisper%20is%20trained%20on%2030,and%20maintaining%20manageable%20computational%20requirements)[\[5\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Performance%20on%20Long%20Audio%3A). Поэтому даже длительные записи (час и более) можно пустить в модель целиком - она внутренне обработает по частям. Однако нужно отслеживать использование памяти: для очень длинных записей может расти нагрузка (Whisper генерирует промежуточные буферы). - **Выход модели:** Whisper возвращает либо полный текст, либо список сегментов с таймкодами и текстом (при использовании трансформер-пайплайна с параметром return_timestamps=True или библиотеки whisperx). Мы можем получить **список сегментов** с временами и текстом. Это позволит нам впоследствии сопоставить сегменты с говорящими (по результатам диаризации).

**Итого:** рекомендуем **OpenAI Whisper Large-v2** для сервиса Transcribe. Он обеспечивает лучшую точность для русского языка. При реализации важно использовать GPU (CUDA) и загрузить модель единожды. Для корректности результатов - добавить шаг постобработки текста (например, заменить "рум" на "RUMA"). В перспективе можно улучшить модель либо через fine-tuning, либо используя функцию prompt bias (подать модель правильные термины в качестве подсказки), если это даст результаты.

## Диаризация (разметка говорящих) - обзор и выбор решения

**Назначение:** Диаризация отвечает на вопрос "кто когда говорил?"[\[6\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=Speaker%20diarization%20is%20the%20process,the%20transcription%20with%20speaker%20labels). На вход подается аудиозапись, а на выходе - список сегментов со временем начала/конца и меткой говорящего (спикера). Эта функция **обогащает транскрипт**: без нее мы знаем "что сказано", но не знаем "кто сказал"[\[6\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=Speaker%20diarization%20is%20the%20process,the%20transcription%20with%20speaker%20labels). Для встреч с несколькими участниками диаризация крайне важна, чтобы разделить реплики по персонам.

**State of the Art:** Современные системы диаризации бывают двух типов - _модульные (cascaded)_ и _энд-то-энд_. Энд-то-энд модели (например, **SortFORMER Diarizer** от NVIDIA) напрямую мапят аудио в метки говорящих[\[7\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=1.%20End). Однако они сложны в обучении и могут ограничивать число спикеров. Более распространены _каскадные_ подходы, где есть несколько шагов[\[8\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=2): 1. **VAD (Voice Activity Detection)** - определение моментов, где в аудио есть речь, а где пауза. 2. **Выделение эмбеддингов говорящих** - каждый фрагмент речи преобразуется в вектор, характеризующий голос (например, модели на основе ECAPA-TDNN). 3. **Кластеризация эмбеддингов** - группировка фрагментов по говорящим (чтобы одинаковые голоса получили одну метку). 4. (Опц.) **TS-VAD / MSDD** - более продвинутые нейросетевые способы уточнить разбиение, учитывая временной контекст (пример - Multiscale Diarization Decoder от NVIDIA).

В промышленности для диаризации часто применяют **Pyannote Audio** (от HuggingFace) и **NVIDIA NeMo**. Пользователь упомянул, что слышал про NeMo - действительно, **NVIDIA NeMo Toolkit** предлагает готовый конвейер для диаризации, сочетающий VAD и MSDD-модели. Подход, подтвержденный исследованиями - связка **Whisper + NeMo**: поскольку Whisper сам не умеет различать говорящих[\[9\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=translation%2C%20it%20does%20not%20inherently,VAD%29%20and), мы можем использовать инструменты NeMo для определения границ речевых фрагментов и их кластеризации по голосам[\[10\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=address%20this%20limitation%2C%20in%20our,VAD%29%20and)[\[11\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Now%2C%20let%E2%80%99s%20understand%20the%20Nemo,to%20find%20the%20occurrence%20of). Этот метод показал высокую точность: NeMo включает _легковесную модель VAD MarbleNet_ для разбивки по тишине и мощную модель **MSDD (Multi-Scale Diarization Decoder)** для определения спикеров, давая точную разметку по времени[\[11\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Now%2C%20let%E2%80%99s%20understand%20the%20Nemo,to%20find%20the%20occurrence%20of)[\[12\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=,msdd_model.diarize). В экспериментах такая связка Whisper+NeMo достигала очень низкой ошибки диаризации (DER около 5.8% на тесте)[\[13\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=print%28f%22Whisper%20Diarization%20Error%20Rate%3A%20%7Bwhisper_der%3A.2), что близко к специализированным сервисам.

Альтернатива - **Pyannote**: сообщество открытых решений предлагает предобученные модели (pyannote.audio), доступные через Hub HuggingFace. Они тоже реализуют VAD + эмбеддинги + кластеризацию под капотом. Pyannote широко известна качеством, но ее модели могут потребовать токена доступа и тоже достаточно тяжелые (модель эмбеддингов ~1GB, VAD поменьше).

Мы склоняемся к использованию **NVIDIA NeMo** для диаризации, учитывая корпоративный контекст и хорошие результаты. NeMo предоставляет готовые конфигурации _inference_ для разных сценариев - например, **«diar_msdd_telephonic»** для телефонных разговоров[\[14\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=match%20at%20L928%20,return%20config), что близко по характеру к встречам/переговорам (2-3 участника, четкая речь). Это можно взять за основу.

**Реализация сервиса Diarize:** Независимо от выбора (NeMo vs pyannote), на этапе старта GPU-сервиса нужно: - **Загрузить модели VAD и диаризации**. В случае NeMo - использовать nemo.collections.asr.models.\*\*ClusteringDiarizer\*\* или NeuralDiarizer API. Например, NeMo позволяет загрузить чекпойнты: VAD (vad_multilingual_marblenet), модель эмбеддинга (ecapa_tdnn или более новая) и MSDD (diar_msdd_telephonic). Можно воспользоваться готовым YAML-конфигом для пайплайна - NeMo имеет пример конфигурации и метод NeuralDiarizer.from_config(cfg)[\[12\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=,msdd_model.diarize). Такой подход автоматически выполнит VAD → эмбеддинги → MSDD. - **Оптимизация загрузки:** как и с Whisper, все тяжелые модели загружаются один раз. VAD и эмбеддинг - относительно легкие (VAD - небольшая CNN, эмбеддинг - ~100M параметров), а MSDD побольше. Все они помещаются в память GPU среднего уровня (несколько ГБ VRAM). - **Обработка запроса:** сервис получает путь к WAV-файлу, читает его (или прямо передает путь в NeMo, если поддерживается). Далее исполняется пайплайн: VAD отдает промежутки речи, эмбеддинги рассчитываются по этим промежуткам, затем MSDD выдаст финальную разбивку с метками. - **Результат:** формируем DiarizationResult - список сегментов {speaker, start_time, end_time}. Спикеров можно обозначать как **Speaker 1, Speaker 2** и т.д. (или Спикер 1 на русском, но лучше нейтрально). Количество говорящих можно не задавать заранее - алгоритм сам определит (MSDD может требовать min/max, но мы можем указать диапазон 1-N или конкретное если знаем число). Для большинства встреч разумно min_speakers=1, max_speakers=5 например. - **Аудио-предобработка:** Диаризация чувствительна к уровню шума и качеству звука. Если известно, что может быть фоновый шум, можно подумать о предварительной фильтрации. NeMo предлагает опцию отключения музыки/шума (в примере был флаг enable_stemming с Demucs для улучшения качества диаризации)[\[15\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=,of%20ram%20enable_stemming%20%3D%20True)[\[16\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=match%20at%20L568%20the%20Meta,using%20the%20original%20audio%20file). Пока можно не углубляться - начнем с базовой обработки. - **Pyannote (альтернатива):** В случае pyannote мы бы использовали Pipeline.from_pretrained("pyannote/speaker-diarization") (потребует токен) и вызвали pipeline(audio_file). Это тоже вернет объект с аннотациями по спикерам, который можно преобразовать в список сегментов. Pyannote, согласно некоторым тестам, достигает DER порядка 10-15% на реальных данных, что тоже приемлемо, но NeMo MSDD зачастую дает еще лучше при донастройке под домен.

**Выбор:** отталкиваясь от того, что пользователь уже знаком с NeMo, и эта библиотека активно поддерживается NVIDIA, **будем использовать NeMo**. Это требует установить nemo-toolkit\[all\] (довольно большой пакет ~4ГБ с весами) или загрузить необходимые части. Возможно, для начального прототипа, чтобы не утяжелять, начнем с pyannote (меньше зависимостей) - но с прицелом перейти на NeMo MSDD для продакшена. В **списке задач** отметим исследование и сравнение Pyannote vs NeMo, но реализацию выполним на том, что быстрее дадим результат (вероятно, pyannote pipeline, если получить доступ к весам, либо NeMo с готовой конфигурацией).

**Примечание по интеграции:** Диаризация сервису тоже передается _путь к аудио файлу_. Это значит, что **GPU-сервис должен иметь доступ к папке с файлами** (например, data/raw/). При разворачивании нужно либо монтировать общий volume, либо использовать сетевое хранилище. Это важно: без доступа к содержимому WAV-файла диаризация не сработает. (В альтернативной архитектуре можно было бы стримить аудио по gRPC, но мы придерживаемся подхода с общим хранилищем для упрощения.)

## Суммаризация встречи - модель и интеграция LLM

**Подход:** После получения полного текста расшифровки необходимо сгенерировать краткое **резюме встречи**. Здесь наиболее эффективно применение больших языковых моделей (LLM). В компании уже развернута собственная LLM с OpenAI-совместимым API - упоминается **Qwen-3** (вероятно, модель семейства Qwen от Alibaba). Предположительно, это мощная модель (сопоставима с GPT-3.5) и ее качества достаточно[\[17\]](https://github.com/QwenLM/Qwen3#:~:text=QwenLM%2FQwen3%20,and%20intelligent%20systems%20to%20date). Поэтому мы можем использовать существующий сервис: отправлять текст транскрипта на этот API и получать summary.

**Альтернативы:** Если по каким-то причинам внешний LLM API недоступен или требуется офлайн-решение, можно рассмотреть модели вроде **BART/RuGPT3** или специализированные модели суммаризации. Однако в открытом доступе нет явно превосходящих LLM решений для разговорных данных на русском. Есть модели для новостных или научных текстов (e.g. ruT5, mBART), но они не обучены на диалогах и показывают усредненные результаты. Практика показывает, что **универсальные LLM (GPT-3.5/4, Llama2, Qwen и т.д.)** дают очень хорошие суммаризации встреч, особенно если их правильно промптнуть (например: _"Суммируй следующую беседу, выдели ключевые решения и задачи..."_).

**План работы суммаризатора:** - **Вход:** полный транскрибированный текст встречи (объединение всех сегментов). - **Проблема контекста:** Длина расшифровки может быть большой (часовая встреча - десятки тысяч токенов). Нужно учесть ограничение контекстного окна модели. Если Qwen-3 имеет контекст ~8k токенов, а транскрипт больше - надо разбивать. Решение: **chunking + iterative summarization**. Например, разделить текст встречи на куски по ~5 минут речи и обрабатывать их последовательно. - **Стратегия суммаризации с чанками:** Можно просуммировать каждый кусок отдельно, а потом суммаризировать суммаризации (или просто объединить их). Либо использовать подход "sliding window": пройтись по тексту, постепенно генерируя сводку. Практический простой метод - разбить на N частей, получить N кратких саммари, затем отдать их модели для финального объединения. Такой многоэтапный процесс широко применяется, например: команда PressW описывает, как они **режут транскрипт на части, LLM обрабатывает куски, затем результаты объединяются**[\[18\]](https://pressw.ai/case-studies/llm-powered-automated-meeting-summarizer#:~:text=Our%20team%20wanted%20to%20be,read%20through%20at%20any%20time). - **Выход:** краткий текст (пару абзацев) с основными моментами встречи. Можно также дополнительно попросить LLM сгенерировать пункты действий или вопросы (как в примере PressW), но для начала достаточно общего резюме. - **Внедрение:** Наш Summarize сервис будет вызывать LLM API. Необходимо настроить URL и ключ API через переменные окружения (например, LLM_API_URL, LLM_API_KEY). Формат запроса - OpenAI API совместимый, т.е. JSON с полем prompt или messages. Qwen-3 совместим с ChatGPT API, значит, можно использовать стандартную библиотеку openai в Python, указав api_base URL компании и соответствующий ключ. - **Оптимизация:** Генерацию резюме можно делать **после завершения распознавания** (то есть не в реальном времени). Это совпадает с требованием - _"суммаризация только когда запись закончена"_. Мы будем запускать ее **последним этапом** pipeline, уже после диаризации. Скорость не критична - даже если резюме строится несколько секунд, это приемлемо, пользователь все равно получает его в конце. Если нужно ускорить, можно уменьшить объем входного текста (например, пропуская явные паузы или очень тривиальные фразы). - **Язык и стиль:** Для русскоязычных встреч логично генерировать резюме на русском. Надо в prompt явно указывать язык вывода. Qwen-3 должна поддерживать русский, но стоит проверить. Также определимся со стилем: возможно, деловой официальный тон. Это можно зафиксировать инструкцией в prompt. - **Качество:** Согласно опыту, LLM хорошо идентифицирует ключевые темы, но может потерять детали. Мы можем протестировать на примерах. Если качество **Qwen-3** нас устраивает (сказано, что устраивает), то этого достаточно. Если нет - можно попробовать более крупную модель (внешний GPT-4) или эксперименты с prompt (например, сначала попросить список тезисов, потом сформировать из них абзац).

**Примечание:** Суммаризация будет реализована как отдельный сервис **Summarize** с интерфейсом gRPC. Но технически, возможно, вместо выделенного GPU ей хватит CPU или обычного сервера, так как LLM вызывается по API. Однако для единообразия архитектуры (и т.к. в будущем можно разместить локальную модель на GPU) мы все равно оформим это как микросервис с gRPC. Он не будет требовать тяжелой инициализации модели, просто будет ретранслировать запрос к API. В перспективе, если захотим заменить внешнюю LLM на локальную (например, разместить Qwen-7B на нашей GPU-ноде), мы сможем это сделать внутри этого сервиса, прозрачно для остальной системы.

## Потоковая расшифровка (аудиострим) - долгосрочная перспектива

Пользователь упомянул **возможность потокового распознавания аудио (streaming)** для "вау"-эффекта. Это означает вывод транскрипции _в реальном времени_ по мере звучания речи, до завершения записи. Действительно, такие возможности есть (например, Whisper можно запустить на поток с небольшим буфером, или использовать специальные стриминговые модели ASR). Однако: - Реализовать это сложно: понадобятся механизмы буферизации аудио, VAD для отсылки порций в модель, и поддержка частичного вывода. Whisper в стандартном виде не оптимизирован для стриминга (он работает с 30-секундными фрагментами), но проекты как **WhisperX** или **NVIDIA Riva** могут обеспечить стриминг. - У нас пока нет такого требования на Phase 1; важнее получить правильный итоговый транскрипт и резюме по завершении записи. - Тем не менее, мы можем заложить **фундамент**: например, делать SSE-сообщения с частичными результатами (сейчас pipeline уже отдает сегменты Whisper по мере готовности через SSE). Это фактически имитирует стриминг после загрузки файла - UI видит текст постепенно. Для live-аудио (с микрофона) можно сделать в будущем: frontend будет отправлять аудио пакетами, а backend - реализует WebSocket или другой SSE, но это отдельный большой кусок работы.

Мы отнесем **стриминговое распознавание** в список будущих улучшений (низкий приоритет). В текущем подходе дождемся окончания записи (файл загружен целиком) и затем выполним все шаги офлайн. Такой режим проще для начального релиза и соответствует требованию: _"суммаризация точно только когда запись закончена"_.

Отметим, что даже без live-стриминга наш сервис уже работает асинхронно через SSE: после загрузки файла сразу начинается обработка, и пользователь видит прогресс (постепенно появляются расшифрованные фрагменты, затем диаризация, потом итоговое резюме). Это достаточно интерактивно.

## Архитектура GPU-сервисов и запуск

**Единый репозиторий:** Код GPU-сервисов мы расположим в том же репозитории, но, вероятно, в отдельной директории (например, gpu_services/). Хотя эти сервисы могут разворачиваться на другой машине или контейнере, хранение кода вместе упростит синхронизацию версий протоколов и тестирование.

**Микросервисы vs Монолит:** Каждая задача (ASR, Diarization, Summarization) может быть вынесена в отдельный сервис (контейнер). Это хорошо с точки зрения масштабирования - можно, скажем, запустить несколько копий ASR-сервиса, если он самый тяжелый, или разместить их на разных GPU. Изначально так и планировалось: в **docker-compose.gpu.yml** упомянуты три контейнера - asr, speaker, summarizer[\[19\]](https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/README.md#L21-L29). Однако сейчас клиентская часть (FastAPI) ожидает **один адрес** GPU (пара GPU_GRPC_HOST:PORT). Чтобы три сервиса заработали, есть два варианта: 1. **Единый gRPC-сервер**: объединить три сервиса в одном приложении (т.е. один процесс, регистрирующий 3 gRPC сервиса на одном порту). gRPC это поддерживает - можно объявить TranscribeServicer, DiarizeServicer, SummarizeServicer и все добавить на один grpc.Server. Тогда GPU_GRPC_HOST/PORT указывает на этот сервер, и на нем доступны три метода. Этот подход проще для конфигурации (ничего менять не надо в Settings), но сложнее в коде (один сервис будет загружать и Whisper, и NeMo, и занимать много ресурсов). 2. **Отдельные сервисы/адреса**: запустить 3 разных сервера (например, на портах 50051, 50052, 50053). Тогда нам придется научить FastAPI использовать разные хосты/порты для каждого типа клиента. Сейчас фабрика create_grpc_client не умеет этого - она берет один GPUSettings. Можно расширить GPUSettings полями для каждого сервиса (напр. ASR_HOST/PORT, DIAR_HOST/PORT, ...), или задать конвенцию портов (например, тот же хост, порты +1 +2). Это потребует небольших изменений, но выполнимо.

С точки зрения **развертывания**, вероятно правильнее _отдельные контейнеры_. Тогда можно даже на разных машинах их держать (например, ASR и diarization требуют GPU, а summarizer может на CPU). Чтобы при этом не усложнять сейчас код, возможен компромисс: **сделать единый сервер для dev**, а потом расширить для прод. Но лучше сразу заложить гибкость.

Мы поступим так: - Реализуем код сервисов модульно (файлы transcribe_server.py, diarize_server.py, summarize_server.py). Каждый из них сможет запускаться отдельно (слушая свой порт). - Добавим в .env возможность задавать разные порты (например, GPU_ASR_PORT, GPU_DIAR_PORT, GPU_SUMM_PORT). По умолчанию можно делать 50051, 50052, 50053. - В GPUSettings можно пока оставить один хост/порт, а порты для других считать как сдвиг (или добавить параметры). - Скорректируем create_grpc_client: например, если service == 'diarize', порт = базовый + 1, и т.д. Или лучше сразу передавать конкретный адрес.

В **первой итерации** можем, однако, запустить всё на одном сервере (в одном процессе) - это упростит отладку. Для этого в коде сервиса можно зарегистрировать все три сервиса на один grpc.Server(). Этот "монолитный" GPU-сервис будет занимать один GPU и выполнять все. Если GPU мощный (например, 24GB), он потянет и Whisper, и NeMo одновременно. Но в будущем, при росте нагрузки, вероятно разделим. Поэтому в задачах отметим и вариант объединенного сервера, и подготовку к разделению.

**Docker и GPU:** Для запуска сервисов на GPU в Docker Compose нужно: - Использовать флаг --gpus all (или в docker-compose.yml указать deploy.resources.reservations.devices с GPU). Мы настроим infra/docker-compose.gpu.yml соответствующе. - Образ для сервисов - сделаем свой Dockerfile, на базе, например, nvidia/cuda:12.2.0-runtime-ubuntu20.04 + Python, устанавливаем PyTorch с CUDA, huggingface, nemo, etc. Это образ будет общий или можно раздельно оптимизировать (например, summarizer не требует NeMo, а diarize не требует transformers). Но для простоты можно один Dockerfile для все трех (они будут из одного образа, просто запускаются с разными командами). - Смонтировать том с аудио: - ./data/raw:/app/data/raw для всех контейнеров, чтобы пути совпадали. - Прописать сеть, чтобы backend мог обращаться по имени сервисов (e.g. asr:50051 etc). - Сертификаты TLS: если будет mTLS, надо прокинуть файлы ключей в контейнеры и настроить серверы использовать их (grpc-сервер с SSL). Пока можно ограничиться insecure (VPN-туннель защищает). Но оставим возможность TLS - backend уже умеет проверять, нужно чтобы серверы тоже могли принять mTLS. Вернемся к этому, когда реальные сертификаты будут.

**Обработка ошибок и таймауты:** Интеграция с реальными сервисами требует учесть нештатные ситуации: что, если распознавание или диаризация займет слишком долго или упадет? FastAPI SSE у нас работает асинхронно, но потенциально долго (минуты). Надо: - Задать **таймаут** на стороне gRPC клиента? grpc-aio позволяет указать timeout при вызове. Можно установить, например, 5 минут на транскрипцию, 2 минуты на диаризацию, 1 минуту на суммари. Либо оставить по умолчанию (бесконечно) и просто ждать. - Если сервис вернул ошибку (exception), gRPC передаст код. Наш pipeline сейчас не обрабатывает ошибки - стоит добавить try/except вокруг вызовов .run() и .stream_run() и в случае ошибки логировать и пробрасывать событие типа error в SSE, а также менять статус встречи на "failed". Добавим это как задачу.

**Тестирование:** Нам понадобятся интеграционные тесты, хотя бы вручную: после поднятия сервисов на GPU, прогнать файл через /upload и проверить, что мы получили осмысленную расшифровку с метками и резюме. Автоматизировать можно на коротком аудио (несколько секунд, 1-2 фразы) - чтобы и Whisper, и диаризация отработали быстро. Для этого можно включить такой файл в репозиторий (или генерировать). В CI, конечно, мы не сможем запустить GPU, но локально разработчик сможет проверить.

Необходимые зависимости (с указанием версий): для реализации понадобятся Python 3.10+ и следующие библиотеки/инструменты: PyTorch 2.x (с поддержкой CUDA 11.8, например torch==2.0.1+cu118), TorchVision/Torchaudio тех же версий (устанавливаются с PyTorch), Transformers >=4.30 (напр. transformers==4.33), пакет Whisper от OpenAI (например, openai-whisper последней версии либо через Transformers модель "openai/whisper-large-v2"), NVIDIA NeMo Toolkit (версия ~1.20, установка pip install nemo_toolkit[asr] для диаризации), клиентская библиотека OpenAI API (например, openai==0.xx) либо HTTP-клиент (requests) для вызова LLM, а также gRPC пакеты (grpcio~=1.75, grpcio-tools~=1.75 для генерации кода из .proto). Дополнительно нужны torchaudio (для чтения аудио), librosa (опционально для обработки аудио), системная утилита FFmpeg (для конвертации аудио при необходимости). Убедитесь, что эти зависимости установлены в среде разработки (при отсутствии GPU можно установить CPU-версии библиотек). В контейнере (см. Dockerfile) эти же зависимости будут установлены в нужных версиях.

Далее приведены конкретные **инструкции и задачи** для реализации GPU-сервисов и интеграции.

## Задачи: Реализация сервисов обработки (TODO_GPU.md)

Ниже перечислены задачи для создания и настройки трех GPU-сервисов: ASR, Diarization, Summarization. Формат аналогичен существующему TODO: каждая задача имеет ID, статус, описание, область, приоритет и зависимости.

**Важно:** На момент начала работ папка для GPU-сервисов пуста - нужно создать структуру. Предлагается завести каталог gpu_services/ в корне, где будут подпапки или файлы для каждого сервиса, а также общий код (например, утилиты, модели, Dockerfile).

Приоритеты: P0 - критически важно для запуска основной функциональности, P1 - важные улучшения, P2 - дополнительные (не блокирующие).

### TODO_GPU.md (задачи по GPU-сервисам)

| ID       | DONE | Task (Описание задачи)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Area                     | Priority | Dependencies                         |
| -------- | ---- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------ | -------- | ------------------------------------ |
| **G1**   | ✗    | **Сервис распознавания речи (ASR) – Whisper Large-v2** – *Разработка разбита на подзадачи G1_x ниже*. Создать отдельный сервис для ASR с использованием модели Whisper.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | GPU/ASR                  | P0       | — (начало работ)                     |
| **G1_1** | ✗    | **Подготовка окружения для ASR** – Установить необходимые зависимости для сервиса распознавания: PyTorch (с поддержкой CUDA), torchaudio, transformers, и пакет Whisper. Убедиться, что модель **Whisper Large-v2** доступна (скачивается через HuggingFace или `openai-whisper`). Настроить конфигурацию: в `.env` или настройках добавить параметр (например, `ASR_MODEL_SIZE`), чтобы можно было указать меньшую модель (например, `medium` или `small`) для тестирования на CPU.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | GPU/ASR                  | P0       | (см. список зависимостей выше)       |
| **G1_2** | ✗    | **Структура кода для ASR-сервиса** – Создать каталог `gpu_services/` в корне репозитория (если еще не создан). В нем создать модуль `gpu_services/asr_service.py`. Сгенерировать (или убедиться в наличии) gRPC-код из `transcribe.proto` – файлы `transcribe_pb2.py` и `transcribe_pb2_grpc.py` (они уже сгенерированы в `backend/app/clients/`). В `asr_service.py` реализовать запуск gRPC-сервера: импортировать классы из сгенерированных файлов (`TranscribeServicer`, функцию регистрации сервера). Создать класс `ASRService(TranscribeServicer)` и определить метод `Run(self, request, context)`. В функции `main` (если запускается как скрипт) создать `grpc.Server`, зарегистрировать `ASRService` через `add_TranscribeServicer_to_server`, слушать порт (из переменных окружения, напр. `50051`).                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | GPU/ASR                  | P0       | protos сгенерированы                 |
| **G1_3** | ✗    | **Загрузка модели Whisper в сервисе** – В конструкторе или при старте `ASRService` загрузить модель **Whisper Large-v2** с помощью HuggingFace Transformers. Использовать половинную точность: `model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2", torch_dtype=torch.float16).to("cuda")` (если GPU доступен). ПредусмотретьFallback: если GPU недоступен (например, в среде тестирования), загрузить модель на CPU (и/или загрузить более маленькую модель, исходя из параметра `ASR_MODEL_SIZE`). Убедиться, что загрузка происходит один раз при запуске сервиса.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | GPU/ASR                  | P0       | G1_1 (библиотеки установлены)        |
| **G1_4** | ✗    | **Реализация метода Run для ASR** – Реализовать логику в `ASRService.Run(request, context)`: открыть аудиофайл по пути `request.path` (файл будет лежать в общей папке, например, `data/raw/<filename>.wav`, доступной и сервису). При необходимости конвертировать аудио к требуемому формату (16 kHz, mono) – можно использовать `torchaudio.load` и, если нужно, `torchaudio.functional.resample` или вызывать ffmpeg. Пропустить аудио через модель Whisper: получить либо полный распознанный текст, либо список сегментов с таймстемпами (например, используя `model.transcribe` из openai-whisper или `pipeline` с `return_timestamps=True`). Если модель выдала сегменты, включить их в результат. **Важно:** после получения текста выполнить пост-обработку – заменить в полученном тексте известные ошибки (например, regex замена `\bрум\b` на **RUMA** и другие оговоренные термины). Подготовить объект ответа `Transcript` (или аналогичный, в протоколе пока только поле `text`) – поместить либо весь текст, либо при необходимости сериализовать сегменты в строку (временно, если протобуф не поддерживает список сегментов). Вернуть результат через gRPC. Добавить логирование времени выполнения распознавания (например, замерять время до/после `model` inference). | GPU/ASR                  | P0       | G1_3 (модель загружена)              |
| **G1_5** | ✗    | **Тестирование сервиса ASR** – Протестировать работу `ASRService`. Локально (без GPU) запустить сервис (он должен уметь работать на CPU с небольшой моделью). Воспользоваться короткой аудиозаписью (несколько секунд речи) для проверки. Написать простой клиент или использовать `grpcurl`/unit-тест, чтобы отправить `AudioRequest(path="...")` и получить распознанный текст. Убедиться, что сервис возвращает осмысленный результат, а пост-обработка заменила специфические слова правильно. При необходимости, откорректировать код.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Tests                    | P0       | G1_4 (сервис реализован)             |
| **G2**   | ✗    | **Сервис диаризации (Speaker Diarization)** – *Разработка разбита на подзадачи G2_x ниже*. Создать отдельный gRPC-сервис для диаризации речи (определение говорящих по аудио).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | GPU/Diarization          | P0       | protos, ASR output format            |
| **G2_1** | ✗    | **Подключение модели диаризации** – Выбрать и подготовить инструмент для диаризации. Базовый вариант – **NVIDIA NeMo** toolkit. Убедиться, что NeMo установлена (`nemo_toolkit[asr]`), а также зависимости (например, `onnxruntime` для VAD, `omegaconf`). Альтернатива – **pyannote.audio** (установить при необходимости, потребуются pre-trained модели и токен HF). На этом шаге следует скачать/разместить необходимые модели: для NeMo – конфигурацию diarization (например, готовый YAML `diar_inference.yaml`) и связанные чекпойнты (VAD, speaker embedding, MSDD) либо использовать встроенный Pipeline. Проверить, что эти ресурсы доступны (например, положить в `gpu_services/models/` или загружать при запуске).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | GPU/Diarization          | P0       | G1_1 (torch, Nemo готовы)            |
| **G2_2** | ✗    | **Структура кода для сервиса диаризации** – Создать модуль `gpu_services/diarize_service.py`. Сгенерировать/проверить gRPC код из `diarize.proto` (файлы `diarize_pb2.py`, `diarize_pb2_grpc.py` есть в проекте). Реализовать класс `DiarizeService(DiarizeServicer)` с методом `Run(AudioRequest)`. В функции запуска (main) поднять `grpc.Server` на отдельном порту (например, 50052) и зарегистрировать `DiarizeService` через `add_DiarizeServicer_to_server`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | GPU/Diarization          | P0       | protos сгенерированы                 |
| **G2_3** | ✗    | **Реализация диаризации в Run** – В методе `Run` сервиса диаризации реализовать полный цикл: получить `request.path` аудиофайла, загрузить аудио (при необходимости привести к нужному формату, как и в ASR). Выполнить диаризацию с помощью выбранной модели. Для NeMo: настроить `NeuralDiarizer` через YAML-конфиг – загрузить конфиг (OmegaConf), указать путь к аудио, пути к моделям, и вызвать diarizer.model.diarize() либо воспользоваться готовым методом (в новых версиях может быть `ClusteringDiarizer` класс). Для pyannote: загрузить pre-trained пайплайн, вызвать `pipeline(audio_path)`. Результат диаризации – список сегментов с метками говорящих и временами. Сформировать ответ `DiarizationResult`: заполнить список `segments` (поля start, end, speaker). Присвоить **метки говорящих**: например, "Speaker 1", "Speaker 2", … в порядке их первого появления в аудио (если NeMo/pyannote не присваивает автоматически). Если в аудио один говорящий, вернуть один сегмент покрывающий все аудио (Speaker 1). Учесть, что диаризация может работать долго на длинных файлах – при необходимости, добавить логирование прогресса (например, вывести сообщение после завершения VAD или каждого этапа).                                                             | GPU/Diarization          | P0       | G2_1 (модели готовы)                 |
| **G2_4** | ✗    | **Тестирование сервиса диаризации** – Проверить работу `DiarizeService` на небольшом аудио. Например, взять отрывок разговора с 2 спикерами длиной ~30 секунд. Запустить сервис локально (можно на CPU, хотя медленно – зато короткий файл). Отправить gRPC-запрос `AudioRequest` и дождаться `DiarizationResult`. Убедиться, что возвращается список сегментов с разными `speaker` (как минимум "Speaker 1", "Speaker 2") и корректными временными метками порядка величины длительности файла. При отсутствии доступной GPU для теста, можно временно подключить более простую модель (например, pyannote с небольшими моделями) или сократить файл до 10 сек. При успешном тесте – диаризация работает.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Tests                    | P0       | G2_3 (сервис реализован)             |
| **G3**   | ✗    | **Сервис суммаризации (LLM)** – *Разработка разбита на подзадачи G3_x ниже*. Создать сервис для генерации резюме встречи на основе полного текста транскрипта.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | GPU/Summarization        | P0       | protos, ASR текст готов              |
| **G3_1** | ✗    | **Структура кода для сервиса суммаризации** – Создать модуль `gpu_services/summarize_service.py`. Убедиться в наличии сгенерированных `summarize_pb2.py` и `summarize_pb2_grpc.py`. Реализовать класс `SummarizeService(SummarizeServicer)` с методом `Run(TextRequest)`. В main-функции запустить gRPC-сервер на порту (например, 50053) и зарегистрировать сервicer через `add_SummarizeServicer_to_server`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | GPU/Summarization        | P0       | protos сгенерированы                 |
| **G3_2** | ✗    | **Реализация вызова LLM в Run** – В методе `Run(self, request, context)` получить `request.text` – полный транскрипт встречи. Реализовать обращение к внешней LLM (в компании имеется развёрнутая модель, совместимая с OpenAI API, например **Qwen-3**). Получить URL/endpoint и API-ключ из конфигурации (переменные окружения `LLM_API_BASE`, `LLM_API_KEY`, добавить их в `.env` и `GPUSettings`). В простейшем случае отправлять весь текст одним запросом (через библиотеку `openai` или HTTP POST). Задать параметры: модель (если требуется), **низкую температуру** (около 0.2-0.3 для консистентности результата). Получить от API сгенерированный summary.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | GPU/Summarization        | P0       | G1_1 (библиотека openai установлена) |
| **G3_3** | ✗    | **Обработка длинного текста (chunking)** – Если транскрипт очень большой (например, более ~5000 слов или близок к ограничению контекста ~4096 токенов), реализовать логику разбиения: разбить текст на несколько частей (например, ~N символов или по абзацам, чтобы каждая часть помещалась в контекст). В цикле вызвать LLM API для каждой части с промптом вроде *«Суммируй следующий фрагмент: ...»*. Полученные частичные summary соединить. Если частей было несколько, затем отправить объединенный итог (или весь исходный текст + частичные резюме) ещё раз в LLM для финального суммирования всего контента. Этот многоступенчатый процесс описать в коде с комментариями, чтобы было понятно, как он работает.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | GPU/Summarization        | P0       | G3_2 (базовый вызов готов)           |
| **G3_4** | ✗    | **Обработка ошибок и кодировка** – Добавить обработку ошибок при вызове LLM API: обернуть запросы в try/except, при исключениях или HTTP 5xx ответах логировать ошибку и возвращать gRPC-статус **UNAVAILABLE** или соответствующий код. Убедиться, что текст на русском не теряет кодировку (при использовании библиотеки openai это обычно автоматически, но если через requests – правильно указать `.encode('utf-8')` при необходимости). После получения итогового резюме сформировать объект ответа `Summary` (с полем text) и вернуть через gRPC.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | GPU/Summarization        | P0       | G3_3 (логика суммирования готова)    |
| **G3_5** | ✗    | **Тестирование сервиса суммаризации** – Проверить `SummarizeService`: для теста можно не гонять реальную большую LLM-модель, а использовать небольшой текст. Запустить сервис (можно без GPU, так как он не использует локальных моделей). Вызвать метод `Run` с небольшим `TextRequest.text` (например, пару абзацев осмысленного текста) и убедиться, что приходит осмысленный ответ (если доступен реальный API). Если API недоступен в тестовой среде, можно замокать вызов LLM (например, временно возвращать фиксированный текст в Run) – важно проверить, что gRPC-взаимодействие работает.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Tests                    | P0       | G3_4 (сервис реализован)             |
| **G4**   | ✗    | **Docker-окружение для GPU-сервисов** – *Разбито на G4_x ниже*. Подготовить контейнеры и оркестрацию для всех новых сервисов (ASR, Diarization, Summarization).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | DevOps/Container         | P0       | G1, G2, G3 (код готов)               |
| **G4_1** | ✗    | **Написание Dockerfile** – Создать Dockerfile (например, `gpu_services/Dockerfile`) для сборки образа с GPU-сервисами. Использовать базовый образ с **CUDA** и Python (например, `nvidia/cuda:11.8.0-runtime-ubuntu20.04` с установкой Python 3.10, либо образ `nvidia/cuda:12.2.0-runtime-ubuntu20.04` – тогда убедиться, что ставится совместимая версия PyTorch). В Dockerfile установить все требуемые зависимости: командой `pip install` установить `torch`, `torchvision`, `torchaudio` (с нужной CUDA, через `--extra-index-url https://download.pytorch.org/whl/cu118`), `transformers==<версия>`, `git+https://github.com/openai/whisper.git` (или `openai-whisper`), `nemo_toolkit[asr]`, `openai` (клиент для API) или `requests`, а также `protobuf`, `grpcio`, `grpcio-tools`. При необходимости установить системные пакеты: например, `ffmpeg` (через apt) для обработки аудио. Убедиться, что версии совпадают с требованиями (PyTorch и CUDA).                                                                                                                                                                                                                                                                                                                            | DevOps/Container         | P0       | G1_1 (список зависимостей)           |
| **G4_2** | ✗    | **Конфигурация docker-compose** – Обновить файл `infra/docker-compose.gpu.yml` для запуска всех трех сервисов. Для каждого сервиса (asr, speaker, summarizer) добавить сервис в YAML: использовать образ, собранный Dockerfile-ом G4_1. В `command` или `entrypoint` указывать запуск соответствующего сервиса, например: `python -m gpu_services.asr_service` для ASR, и аналогично для других. Назначить каждому сервису порт (например, asr – 50051, diarization – 50052, summary – 50053) и пробросить их наружу (`ports:` в compose). Смонтировать общий том с аудиофайлами: например, монтировать локальную папку `./data/raw` в контейнеры на тот же путь (чтобы сервисы находили файлы по одинаковому пути). Если планируется TLS, продумать монтаж сертификатов (например, папку `certs/` в контейнер). Для доступа к GPU в Docker Compose добавить для каждого сервиса параметр `deploy.resources.reservations.devices` с capability `gpu` (или запустить с флагом `--gpus all`).                                                                                                                                                                                                                                                                                                 | DevOps/Container         | P0       | G4_1 (образ готов)                   |
| **G4_3** | ✗    | **Тестирование контейнеров** – Запустить локально `docker-compose -f infra/docker-compose.gpu.yml up` и убедиться, что все три контейнера стартуют. Проверить лог вывода: ASR сервис должен сообщить о загрузке модели Whisper, Diarization – о загрузке моделей диаризации, Summarization – о запуске и ожидании запросов. Затем запустить сквозной тест: поднятый FastAPI-бекенд (в режиме grpc-клиентов) и загруженный через API аудиофайл. Убедиться, что бекенд успешно соединяется с gRPC-сервисами внутри compose и получает результаты. Если какие-то переменные (например, HOST) настроены неправильно – скорректировать Compose или .env (в docker-compose можно задать `GPU_GRPC_HOST` = имя сервиса контейнера, например, `asr`). После успешного запуска – контейнеризация готова.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | DevOps/Container         | P0       | G4_2 (compose настроен), I3          |
| **G4_4** | ✗    | **Документация запуска GPU-сервисов** – Обновить README (или docs) проекта: описать, как запустить GPU-версии сервисов. Добавить инструкцию про `docker-compose.gpu.yml`: например, требование установить NVIDIA Docker runtime, команду запуска, переменные окружения (`GPU_GRPC_HOST`, `GPU_GRPC_PORT` или отдельные порты). Отметить, что в локальной отладке без GPU можно использовать небольшие модели или мок-сервисы. Также указать, что в продакшне возможно разнести сервисы на разные GPU/машины (и тогда нужно настроить переменные HOST/PORT соответствующим образом).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Docs                     | P0       | G4_3 (все запущено локально)         |
| **G5**   | ✗    | **Тонкая настройка точности ASR (доменные слова)** – Проанализировать качество распознавания на доменном словаре (например, название **RUMA** и другие специфичные термины). На этом этапе, после запуска базового сервиса, провести тесты на нескольких известных записях. Если замечены искажения (как "рум" вместо "RUMA"), дополнить пост-обработку. Составить список проблемных слов и их правильных форм. Реализовать дополнительную логику замены/нормализации: например, помимо простого `RUMA` возможно добавить другие термины, аббревиатуры, которые часто искажаются. Изучить возможность **Whisper prompt**: подача правильных терминов в качестве подсказки модели (если API Whisper или HF Model позволяет). Если улучшения незначительны, оставить простую замену. Задача считается выполненной, когда на тестовых записях ключевые термины распознаются правильно. Описать внесенные изменения (в коде комментариями или отдельной заметкой в документации).                                                                                                                                                                                                                                                                                                               | GPU/ASR Quality          | P1       | G1 (базовый ASR работает)            |
| **G6**   | ✗    | **Сравнение Pyannote vs NeMo (опционально)** – *Исследовательская задача.* Попробовать альтернативный подход к диаризации с библиотекой **pyannote.audio** и сравнить с текущей (NeMo) по качеству и скорости. Установить `pyannote.audio` и необходимые модели (например, `pyannote/speaker-diarization@2023.07` через HuggingFace, потребует токен). Взять те же тестовые аудио, прогнать диаризацию через pyannote Pipeline. Сравнить результаты сегментации и метки: насколько отличаются от NeMo, есть ли ошибки (слияние говорящих, пропуски). Замерить время выполнения и потребление памяти для каждого подхода. Сделать вывод: если pyannote дает сопоставимое качество и проще в использовании, можно в будущем переключиться на него (например, для среды без мощных GPU). Результаты эксперимента оформить в документе (например, дополнить `docs/researches/speech_stack_research.md` или создать новый отчет) с выводами. **Замечение:** эта задача не блокирует основной функционал (низкий приоритет, P3).                                                                                                                                                                                                                                                                  | GPU/Diarization Research | P3       | G2 (основная диаризация готова)      |
| **G7**   | ✗    | **Единый GPU-сервис (режим all-in-one)** – *Опционально, для удобства разработки.* Реализовать возможность запуска всех функций обработки в одном процессе (на одном gRPC сервере). Создать модуль `gpu_services/unified_server.py`, где импортировать `ASRService`, `DiarizeService`, `SummarizeService`. Запустить один `grpc.Server` и зарегистрировать все три servicer на разных gRPC-классах (т.е. один процесс слушает, например, порт 50051 и обрабатывает три типа запросов). Добавить флаг в настройки, напр. `GPU_SINGLE_PROCESS=true`, чтобы переключаться на этот режим (в FastAPI можно тогда использовать один хост:порт). Этот режим удобен для локального тестирования, если на одной машине/GPU память позволяет загрузить сразу Whisper и NeMo. Проверить потребление памяти: Whisper Large (FP16) ~5-6GB, NeMo модели ~2-3GB, итого ~8GB – поместится на 16GB GPU. Если нет, документировать, что единый режим только для мощных карт. Протестировать unified-сервис локально: должен работать как три в одном. Оставить в документации примечание, что в production лучше разделенные сервисы, а unified – для отладки.                                                                                                                                                | GPU/All-in-One           | P2       | G1, G2, G3 (сервисы сделаны)         |


## Задачи: Интеграция GPU-сервисов в систему (TODO_INTEGRATION.md)

Теперь, когда определены задачи по созданию самих сервисов, необходимо спланировать интеграцию их с основным приложением (FastAPI бэкендом) и фронтендом. Эти задачи охватывают изменения конфигурации, оркестрацию, пост-обработку результатов и обновление пользовательского опыта (например, отображение говорящих). Итоговый список оформлен как отдельный TODO_INTEGRATION.md.

**Цели интеграции:** - Переключить Backend с mock-клиентов на реальные gRPC вызовы. - Обеспечить доступность GPU-сервисов (настройка адресов, безопасность). - Обработать новые данные (например, сегменты диаризации) - возможно, сохранять их и использовать на фронтенде. - Улучшить выдачу для пользователя: имена говорящих в тексте, и т.д. - Оставить возможность работать в режиме разработки без GPU (т.е. fallback на mock).

Приоритеты интеграционных задач также размечены: P0 для критически необходимых, P1 для важных улучшений, P2/P3 для дальнейших шагов.

### TODO_INTEGRATION.md (задачи по интеграции)

| ID       | DONE | Task (Описание задачи)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Area                    | Priority | Dependencies                                  |
| -------- | ---- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------- | -------- | --------------------------------------------- |
| **I3**   | ✗    | **Переключение бекенда на реальные GPU-сервисы** – *Разбито на подзадачи I3_x ниже*. Изменить бекенд (FastAPI) так, чтобы он вызывал gRPC API новых сервисов вместо мок-реализаций.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Backend/GPU Integration | P0       | G1, G2, G3 (сервисы запущены)                 |
| **I3_1** | ✗    | **Настройки подключения к нескольким сервисам** – Обновить класс настроек `GPUSettings` в бекенде. Сейчас он имеет `GPU_GRPC_HOST` и единый `GPU_GRPC_PORT`. Добавить отдельные поля для портов (и при необходимости хостов) каждого сервиса: например, `ASR_GRPC_PORT`, `DIAR_GRPC_PORT`, `SUMM_GRPC_PORT` (если хост общий, достаточно одного `GPU_GRPC_HOST`). Прописать эти переменные в `.env.example` с значениями по умолчанию (например, 50051/50052/50053). Если принято решение держать один порт с unified-сервером, то можно альтернативно поддержать оба режима (например, если `GPU_SINGLE_PROCESS=true`, использовать `GPU_GRPC_PORT`, иначе – отдельные).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Backend/Config          | P0       | .env, G4 (решение о портах)                   |
| **I3_2** | ✗    | **Фабрика gRPC-клиентов с раздельными адресами** – Модифицировать фабричную функцию `create_grpc_client` в бекенде (в модуле `app/grpc_client.py` или `app/clients/grpc_clients.py`). Ранее она, вероятно, смотрит на `GPUSettings` и создает либо MockClients, либо реальные gRPC Stubs. Теперь нужно учесть несколько сервисов: в зависимости от типа клиента (`transcribe`, `diarize`, `summarize`) подключаться на разные порты. Например, можно в `GPUSettings` добавить метод или свойство, возвращающее порт по имени сервиса, или в самой функции сделать if/elif. Реализовать создание `grpc.Channel` на `GPU_GRPC_HOST:ASR_GRPC_PORT` для ASR (и аналогично для других), затем возвращать сгенерированный Stub (например, `TranscribeStub(channel)` и т.д.). Убедиться, что при инициализации приложения создаются клиенты для всех сервисов (или по требованию) и что они используют правильные адреса.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Backend                 | P0       | I3_1 (настройки готовы)                       |
| **I3_3** | ✗    | **Переключение с mock на grpc** – Изменить конфигурацию по умолчанию, чтобы приложение использовало реальный режим. Вероятно, в `pipeline.py` или в `settings` есть параметр типа `GRPC_CLIENT_TYPE` = 'mock' сейчас. Переключить на 'grpc' по умолчанию для боевого запуска. При этом обеспечить, что можно обратно переключиться на mock (например, для тестов) – то есть не удалить mock-клиенты совсем. Простым способом: оставить переменную, но в `.env.example` выставить 'grpc'. Протестировать запуск бекенда: он должен пытаться подключаться к указанным gRPC сервисам (если они не запущены – будет ошибка соединения, это нормально на этапе разработки). Убедиться, что при запущенных контейнерах (из G4_3) backend успешно вызывает методы (можно временно добавить логи или использованные порты в вывод для уверенности).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Backend                 | P0       | I3_2 (клиенты реализованы)                    |
| **I3_4** | ✗    | **Обеспечение режима без GPU (fallback)** – Позаботиться, чтобы система могла работать **без** запущенных GPU-сервисов (например, в тестовой среде CI/CD). Для этого убедиться, что переменная `GRPC_CLIENT_TYPE=mock` продолжает переключать клиентов на mock-объекты. Проверить, что ни одна из вышевыполненных модификаций не сломала режим mock: запустить тесты бекенда, они должны проходить (там, где используем MockTranscribeClient и пр.). Возможно, имеет смысл в коде `create_grpc_client` сделать так, что при отсутствии указанных GPU_HOST/портов или при явном указании типа 'mock' – используются моки. Таким образом, разработка и тестирование могут выполняться без поднятия тяжелых сервисов.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Backend                 | P0       | I3_2                                          |
| **I4**   | ✗    | **Общий доступ к аудиофайлам** – Настроить единое хранилище аудио между backend и GPU-сервисами. Локально (в Docker Compose) это уже учтено (том `data/raw` смонтирован в контейнеры). Необходимо проверить в конфигурации бекенда путь сохранения загруженных файлов. Например, бекенд сохраняет файл в `data/raw/<name>.wav` (см. `RAW_AUDIO_DIR` в настройках). Убедиться, что в контейнерах GPU-сервисов этот путь соответствует тому же самому физическому файлу (благодаря общему тому). Если есть рассогласование (например, бекенд сохраняет вне `/app/data`), исправить либо путь сохранения, либо путь монтирования. В `.env` задать `RAW_AUDIO_DIR=/app/data/raw` (путь внутри контейнера). **В продакшене**, где сервисы могут быть на разных машинах, предусмотреть использование общего хранилища: например, NAS или S3. Это выходит за рамки разработки, но в документации отметить: *GPU-сервисы должны иметь доступ к тем же аудиофайлам, что и бекенд*.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Infra/Storage           | P0       | deployment env, G1-G3                         |
| **I5**   | ✗    | **Безопасное соединение (TLS/mTLS) между сервисами** – *Разбито на I5_x ниже*. Обеспечить шифрование трафика gRPC между бекендом и GPU-сервисами, согласно требованиям безопасности.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Backend/GPU Security    | P1       | G1-G4, I3 (базовая работа)                    |
| **I5_1** | ✗    | **Генерация сертификатов** – Создать самоподписанные сертификаты для gRPC-сервисов. Для mTLS нужны: сертификат и приватный ключ сервера, сертификат и ключ клиента, а также CA (можно сам CA выступать одновременно сервером). Сгенерировать (с помощью openssl или скрипта) файлы, например: `gpu_server.crt` + `gpu_server.key` для сервисов, `client.crt` + `client.key` для бекенда, и `ca.crt` (общий корневой). Разместить эти файлы (в dev-режиме) в каталоге `certs/` и добавить их в `docker-compose.gpu.yml` как volume (чтобы сервисы могли их прочитать).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Security                | P1       | (внешние инструменты)                         |
| **I5_2** | ✗    | **Настройка TLS на стороне GPU-сервисов** – Изменить код запуска gRPC-серверов (в сервисах ASR, Diarize, Summarize): при создании `grpc.Server()` использовать SSL-сертификат. Например, загрузить `gpu_server.crt` и `gpu_server.key`, вызвать `grpc.ssl_server_credentials(((key, cert),))` и передать в `server.add_secure_port('0.0.0.0:50051', creds)`. Если нужна проверка клиента (mTLS), передать также `request_client_certificate=True` и CA-корень. Убедиться, что сервисы запускаются без ошибок с включенным TLS (в логах может быть пометка о secure connection). В `.env` ввести флаг `GPU_GRPC_USE_TLS=true/false` – чтобы легко переключать режим. При false – сервер запускается обычным `add_insecure_port`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | GPU Services            | P1       | I5_1 (сертификаты готовы)                     |
| **I5_3** | ✗    | **Настройка TLS на стороне бекенда** – Обновить gRPC-клиенты в бекенде: при создании канала использовать secure_channel с загрузкой сертификатов. В `GPUSettings` уже предусмотрены поля `GPU_GRPC_TLS_CA`, `GPU_GRPC_TLS_CERT`, `GPU_GRPC_TLS_KEY` – заполнить их путями (в .env) до соответствующих файлов (например, внутри контейнера бекенда они тоже будут смонтированы). В коде `create_grpc_client`, при `use_tls=true` создать креденшилы: `grpc.ssl_channel_credentials(ca_cert, client_cert, client_key)` и передавать их в `grpc.secure_channel(host:port, creds)`. Протестировать локально: запустить compose с TLS включенным (в .env `GPU_GRPC_USE_TLS=true` и заданы пути к файлам сертификатов в контейнерах). Бекенд должен успешно подключиться (если нет – смотреть ошибки TLS handshake в логах).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Backend Security        | P1       | I5_2 (сервисы с TLS)                          |
| **I5_4** | ✗    | **Вариант через VPN** – (Опционально) Если предполагается вместо mTLS использовать VPN-туннель, убедиться, что при `GPU_GRPC_USE_TLS=false` система работает по нешифрованному каналу внутри VPN. В документации (`docs/gpu_security.md`) описать оба подхода: что по умолчанию можно прокинуть соединение через VPN, тогда ничего не надо менять в коде, или включить mTLS как выше. Пошагово расписать, как сгенерировать сертификаты и какие переменные выставить.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Infra/Security Docs     | P1       | I5_3                                          |
| **I5_5** | ✗    | **Тестирование безопасного режима** – После настройки TLS, провести тест: поднять сервисы и бекенд с включенным TLS, выполнить загрузку файла и получение расшифровки. Убедиться, что все работает так же, как раньше, и в логах трафик шифруется (явных признаков может не быть, но отсутствие ошибок handshake – уже признак). Также проверить, что при неверных сертификатах соединение не устанавливается (например, указать неправильный client cert – бекенд должен не подключиться). После этого считать безопасное соединение настроенным.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Tests/Security          | P1       | I5_3, I5_4                                    |
| **I6**   | ✗    | **Сохранение и выдача меток говорящих** – *Разбито на подзадачи I6_x ниже*. Интегрировать результаты диаризации в базу данных и в API, чтобы фронтенд мог отображать, кто говорит в каждой части транскрипта.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Backend/API & DB        | P1       | S2 (сохранение транскриптов), G2 (диаризация) |
| **I6_1** | ✗    | **Модификация схемы БД для спикеров** – Проверить модель `Transcript` в базе. Если в модели/миграциях нет поля для указания говорящего, добавить его. Например, добавить колонку `speaker_label VARCHAR` в таблицу transcripts (или числовой speaker_id с отдельной таблицей Speakers – но для упрощения можно строковый ярлык). Использовать Alembic: создать новую миграцию, добавляющую поле `speaker_label` (по умолчанию NULL или 'Speaker 1' для существующих записей). Применить миграцию в локальной БД. Обновить SQLAlchemy модель `Transcript` соответственно.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Backend/DB              | P1       | B4 (базовая миграция)                         |
| **I6_2** | ✗    | **Сопоставление сегментов с говорящими** – В серверной логике обработки (класс `TranscriptService` или `MeetingProcessingService.process`) реализовать объединение результатов ASR и диаризации. У нас есть список сегментов текста (с таймстампами) от Whisper и список сегментов говорящих (с временными интервалами) от диаризации. Реализовать алгоритм: для каждого текстового сегмента определить, какому говорящему он принадлежит. Простой подход – взять временную середину сегмента текста (midpoint = (start+end)/2) и найти в списке диаризации тот сегмент, чьи времена охватывают эту точку (speaker_segment.start <= midpoint <= speaker_segment.end). Назначить текстовому сегменту метку соответствующего спикера. Если сегменты не пересекаются идеально, можно использовать начало текста: если начало текста falls между start-end говорящего, прикрепить его. Для коротких реплик это достаточно точно. Если диаризация вернула только одного говорящего, всем сегментам назначить "Speaker 1".                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Backend/Logic           | P1       | G2 (результаты диаризации)                    |
| **I6_3** | ✗    | **Сохранение транскриптов с говорящими** – После сопоставления, сохранить расшифровку в базу уже с указанием спикеров. В реализации сохранения (Task S2: Persist Transcription Results) нужно при создании каждой записи Transcript задавать поле speaker_label. Обновить соответствующий код (в `TranscriptRepository` или прямо в сервисе, где вызывается репозиторий). Убедиться, что при сохранении каждого сегмента текста теперь сохраняется и колонка speaker_label. При обновлении Meeting статус пометить как завершенный, как и планировалось.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Backend/DB              | P1       | I6_1, I6_2 (спикер определен)                 |
| **I6_4** | ✗    | **Формат ответа API** – Обновить эндпоинт получения деталей встречи `GET /api/meeting/{id}` (Task A10). Ранее он мог возвращать либо цельный текст транскрипта, либо список сегментов (текстовых) – нужно убедиться, что теперь он возвращает вместе с говорящими. Предпочтительно вернуть список объектов, например: `{speaker: "Speaker 1", text: "фраза", start: 0.0, end: 5.2}` для каждого сегмента. Если в текущей реализации `/meeting/{id}` просто отдаёт конкатенированный текст, переработать его: пусть собирает сегменты из TranscriptRepository и формирует JSON-массив сегментов. Добавить поле speaker в эту структуру. Аналогично, если есть отдельное поле summary (сводка) – добавить его в ответ (можно под ключом `summary`). Обновить описание этого эндпоинта в документации, если имеется.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Backend/API             | P1       | I6_3 (данные сохраняются)                     |
| **I6_5** | ✗    | **Обработка SSE (онлайн режим)** – (Опционально, можно упрощенно) Подумать, как передавать метки спикеров в реальном времени через SSE `/stream`. Сейчас SSE события, вероятно, реализованы как: по мере готовности сегмента Whisper отправляется событие `transcript` с текстом. Диаризация же завершится только после полного аудио. В реальном времени трудно сразу метить спикеров. Можно оставить SSE как есть (он отдает текст без спикеров во время обработки), а после завершения (последнего события) фронтенд сделает запрос на полный транскрипт с говорящими. Либо можно по завершении диаризации отправить отдельное событие, например `diarization` с массивом {start, end, speaker} сегментов – и на фронте попытаться сопоставить. Но это сложно синхронизировать. Более простой путь: **сделать фронт запрашиваемым итоговые данные**. Поэтому можно ограничиться тем, что SSE во время обработки не содержит спикеров, а финальный API (из I6_4) их предоставляет. В коде это означает, что можно не трогать логику SSE (кроме, может быть, добавления события `summary`).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Backend/SSE             | P2       | I6_4 (спикеры в данных)                       |
| **I6_6** | ✗    | **Тестирование функции спикеров** – Провести тестирование полного процесса с диаризацией: загрузить аудио с 2+ участниками, дождаться окончания обработки. Затем вызвать GET `/api/meeting/{id}` и проверить, что в ответе сегменты помечены разными `speaker`. Убедиться, что данные в базе сохранены правильно (можно посмотреть напрямую в таблице transcripts). Также проверить, что если аудио с одним говорящим – все сегменты имеют "Speaker 1". И что фронтенд (если уже обновлен в I7) корректно отображает эти метки.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Tests                   | P1       | I6_4, I7 (если UI готов)                      |
| **I7**   | ✗    | **UI: Отображение говорящих на фронтенде** – *Разбито на I7_x ниже*. Обновить интерфейс пользователя, чтобы он показывал, кто произнес каждую часть текста.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Frontend/UI             | P2       | I6 (бекенд отдает спикеров)                   |
| **I7_1** | ✗    | **Transcript Detail с метками** – В компоненте **TranscriptDetail** (страница расшифровки встречи) изменить вывод текста. Ранее он, возможно, показывал цельный текст или список абзацев. Теперь, получая от API массив `{speaker, text, ...}`, отрисовывать их с указанием говорящего. Например: перед текстом каждого сегмента вставить `<b>{speaker}:</b> ` и затем сам текст. Можно стилизовать: каждому уникальному спикеру присвоить свой цвет текста или метку (например, с разными аватарками, но для простоты – цвет/значок). Убедиться, что при рендеринге сохраняется порядок сегментов. Если реализовано событие summary, отобразить резюме отдельным блоком (например, вверху или внизу транскрипта, с заголовком "Summary").                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Frontend/UI             | P2       | I6_4 (API данных готов)                       |
| **I7_2** | ✗    | **Список встреч: участники** – На странице списка встреч (Task F9) добавить информацию о количестве спикеров или именах. Например, в каждой строке списка показывать "Спикеры: 2" если в записи больше одного говорящего. Эти данные можно получить, например, подсчитав уникальные speaker_label в транскриптах встречи (можно хранить это число в таблице Meeting, но проще – на фронте после получения деталей). Поскольку API списка встреч (A9) пока не отдаёт спикеров, можно реализовать следующим образом: при клике на встречу на Detail странице все равно увидим подробности. Поэтому этот пункт не критичен – основное, отобразить в деталях. (Опционально: улучшить API списка, чтобы он возвращал, скажем, поле `speaker_count` или список спикеров, тогда фронт сможет сразу показать).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Frontend/UI             | P3       | A9, I6 (данные о спикерах)                    |
| **I7_3** | ✗    | **Live UI без изменений** – Принять решение насчет отображения говорящих в **режиме реального времени** (SSE поток, компонент F8). Как обсуждалось, метки спикеров в момент стриминга достоверно недоступны. Вероятно, на Phase 1 можно оставить как есть: в реальном времени отображаются сегменты текста слитно. После завершения пользователь может обновить страницу или перейти в детальный просмотр, где уже будет расшифровка с разделением по спикерам. В качестве улучшения можно в live-транскрипте при приходе финального события (summary) либо по таймеру автоматически перезагрузить данные встречи (вызвать API /meeting/{id}) и обновить отображение на разделе деталей. Но это сложнее, можно сделать позже. Поэтому на этапе Phase 2 фронтенд **в live режиме спикеров не показывает**, только в итоговом. В документации/подсказках можно указать это ограничение.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Frontend/SSE            | P3       | I6 (понимание ограничения)                    |
| **I7_4** | ✗    | **Локализация** – При добавлении интерфейсных элементов проверить, не нужны ли новые строки перевода. Например, слово "Speaker" на русском "Спикер". Если использована маркировка "Speaker 1, Speaker 2", можно оставить на английском или перевести. Убедиться, что все новые тексты (например, метка "Summary" если добавлена) вынесены в файлы локализации en/ru и используются через i18n.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Frontend/I18n           | P3       | I7_1 (UI изменен)                             |
| **I7_5** | ✗    | **Тестирование интерфейса** – Протестировать фронтенд с обновлениями: загрузить аудио через UI, дождаться вывода. Убедиться, что на странице **Live Transcript** текст идет без спикеров (как ожидалось), после завершения отображается резюме (если реализовано). Затем открыть эту встречу в **Transcript Detail** – проверить, что перед каждым сегментом стоит правильный спикер (Speaker 1/2/etc.), сегменты разделены корректно, цвета/стили применяются. Проверить на разных сценариях: файл с одним говорящим (должен отображаться только Speaker 1), с несколькими, на английском языке (метки спикеров все равно "Speaker X"). Убедиться, что ничего не ломается (например, длинные имена спикеров или много сегментов не рушат верстку).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Frontend/Test           | P2       | I7_1 (функционал готов)                       |
| **I8**   | ✗    | **End-to-End тестирование и оптимизация** – После интеграции всех компонентов провести сквозной тест на реалистичных данных и при необходимости оптимизировать. Взять длительную аудиозапись встречи (например, 5-10 минут, 2-3 спикера). Загрузить через фронтенд, проследить полный процесс: распознавание (ASR) -> диаризация -> суммаризация -> отображение. Измерить время выполнения каждого этапа и общее время. Если возможно, включить логирование времени в код (например, ASR занял X секунд, диаризация Y, суммирование Z). Оценить производительность: для 10 мин аудио приемлемо, если обработка занимает ~2-3 минуты (реальное время). Если значительно дольше, определить узкие места. Возможные проблемы: **ASR Whisper Large** – самый медленный (близко к х1 реального времени на GPU). Диаризация NeMo обычно быстрее (~0.5x). Суммаризация зависит от длины текста, но обычно несколько секунд. Если ASR слишком медленный, рассмотреть варианты оптимизации: например, использовать модель поменьше (Whisper Medium) ценой точности – можно сделать это на уровне конфигурации (поменяв `ASR_MODEL_SIZE` и перезапустив сервис) и сравнить качество/скорость. Или если доступно несколько GPU – параллельно обрабатывать несколько кусков (реализация сложна). В рамках этой задачи: зафиксировать текущие показатели и обсудить с командой, устраивают ли они. Если есть простые оптимизации (например, увеличить размер chunk в Whisper или отключить ненужные опции) – внести и измерить снова. Конечная цель – убедиться, что система стабильно обрабатывает данные конечной длины, а вывод корректен (текст точный, спикеры правильные, резюме осмысленное). Результаты этого тестирования описать (в виде комментария в документации или отдельного отчета), чтобы иметь референс производительности. | System/Test             | P1       | I3-I7 (вся функциональность)                  |
| **I9**   | ✗    | **План реализации стримингового распознавания** – *Аналитическая задача на будущее*. Описать, какие шаги необходимы для поддержки **потокового ASR в реальном времени** (когда аудио идет непрерывно с микрофона, а текст появляется сразу). Изучить варианты: модель **Whisper** можно заставить работать на стрим (передавая кусочки с overlap), либо интегрировать специализированную систему (**NVIDIA Riva** ASR с realtime, или другие модели). На базе выбранной технологии описать изменения: 1) На фронтенде – использовать WebSocket или WebRTC для передачи аудио чанками, вместо загрузки файла. 2) На бекенде – endpoint, который принимает аудиопоток и порционно отправляет в ASR сервис (в gRPC тоже понадобится стриминг RPC вместо unary). 3) Модификация ASR сервиса – принимать поток аудио и выдавать частичные результаты (это самая сложная часть, Whisper изначально не стримовый, но можно запускать каждые N секунд на накопленном буфере с пересечением). Продумать, как синхронизовать вывод, чтобы не было дублей и пропусков. Также учесть, как диаризация будет работать в стриминговом режиме (возможно, пост-фактум или с задержкой). Все эти моменты собрать в документе (создать `docs/researches/streaming_plan.md`). Пока что реализацию не выполнять, только план: оценить трудоемкость, требования к инфраструктуре (например, возможно, понадобится более быстрая модель ASR для стрима). Этот план позволит взвесить, стоит ли браться за стриминг в следующих фазах.                                                                                                                                                                                                                                                                                                                    | Research/Future         | P3       | (нет прямых зависимостей)                     |


**Примечание:** Нумерация интеграционных задач продолжает ID из предыдущего списка (Part 2) - предыдущие инфратаски дошли до I2, поэтому начинаем с I3. После выполнения этих задач система должна полноценно работать на GPU-сервисах с сохранением транскриптов, что соответствует завершению **Фазы 2 (Реальная обработка аудио)** из плана[\[20\]](https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/implementation_plan.md#L130-L137). Далее можно будет переходить к Фазе 3 (UI улучшения) и другим фазам, уже имея надежный backend-функционал.

Примечание: После выполнения вышеперечисленных задач система завершит Фазу 2 (реализация реальной обработки аудио на GPU). Все аудио будут обрабатываться реальными моделями, результаты сохраняться, фронтенд отображать говорящих. Далее можно переходить к Фазе 3 (улучшения UI, дополнительные фичи), уже имея надежно работающий функционал распознавания.

[\[1\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=OpenAI%20Whisper%3A%20Seq2Seq%20for%20A,utomatic%20Speech%20Recognition) [\[4\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Whisper%20is%20trained%20on%2030,and%20maintaining%20manageable%20computational%20requirements) [\[5\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Performance%20on%20Long%20Audio%3A) [\[9\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=translation%2C%20it%20does%20not%20inherently,VAD%29%20and) [\[10\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=address%20this%20limitation%2C%20in%20our,VAD%29%20and) [\[11\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Now%2C%20let%E2%80%99s%20understand%20the%20Nemo,to%20find%20the%20occurrence%20of) [\[12\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=,msdd_model.diarize) [\[13\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=print%28f%22Whisper%20Diarization%20Error%20Rate%3A%20%7Bwhisper_der%3A.2) [\[14\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=match%20at%20L928%20,return%20config) [\[15\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=,of%20ram%20enable_stemming%20%3D%20True) [\[16\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=match%20at%20L568%20the%20Meta,using%20the%20original%20audio%20file) Whisper Automatic Speech Recognition (ASR) with Diarization

<https://learnopencv.com/automatic-speech-recognition/>

[\[2\]](https://cookbook.openai.com/examples/whisper_prompting_guide#:~:text=Whisper%20may%20incorrectly%20transcribe%20uncommon,can%20use%20prompts%20to) Whisper prompting guide | OpenAI Cookbook

<https://cookbook.openai.com/examples/whisper_prompting_guide>

[\[3\]](https://github.com/openai/whisper/discussions/1268#:~:text=Introducing%20helping%20dictionnary%20with%20acronyms%2C,decide%20properly%20when%20in%20doubt) Introducing helping dictionnary with acronyms, people names, place ...

<https://github.com/openai/whisper/discussions/1268>

[\[6\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=Speaker%20diarization%20is%20the%20process,the%20transcription%20with%20speaker%20labels) [\[7\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=1.%20End) [\[8\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=2) Speaker Diarization - NVIDIA NeMo Framework User Guide

<https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html>

[\[17\]](https://github.com/QwenLM/Qwen3#:~:text=QwenLM%2FQwen3%20,and%20intelligent%20systems%20to%20date) QwenLM/Qwen3 - GitHub

<https://github.com/QwenLM/Qwen3>

[\[18\]](https://pressw.ai/case-studies/llm-powered-automated-meeting-summarizer#:~:text=Our%20team%20wanted%20to%20be,read%20through%20at%20any%20time) LLM powered automated meeting summarizer - PressW AI Solutions

<https://pressw.ai/case-studies/llm-powered-automated-meeting-summarizer>

[\[19\]](https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/README.md#L21-L29) README.md

<https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/README.md>

[\[20\]](https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/implementation_plan.md#L130-L137) implementation_plan.md

<https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/implementation_plan.md>
