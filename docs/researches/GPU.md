date: 2025-09-27

# Обработка аудио: модели, сервисы GPU и интеграция

## Введение и потребности сервиса

Для системы распознавания встреч требуются три основных компонента обработки аудио:

- **Распознавание речи (ASR)** - преобразование аудиозаписи в текст расшифровки.
- **Диаризация (Speaker Diarization)** - определение, кто и когда говорит, то есть разметка текста по говорящим.
- **Суммаризация** - генерация краткого резюме встречи на основе полной расшифровки.

Аудитория преимущественно русскоязычная, поэтому модели должны хорошо работать с русским языком. Качество распознавания и суммаризации является приоритетом, в то время как эффективность (скорость) важна, но не критична - можно обрабатывать запросы последовательно, ставя их в очередь при необходимости. Ниже рассмотрены лучшие на текущий момент подходы для каждого компонента (кроме LLM, т.к. готовая LLM-модель для суммаризации уже доступна) и описаны планы по реализации GPU-сервисов, отвечающих за каждую задачу.

## Распознавание речи (ASR) - выбор модели и загрузка

**State of the Art (русский язык):** Самой точной открытой моделью распознавания речи, поддерживающей русский язык, является **OpenAI Whisper**, особенно ее крупная версия _Whisper Large-v2_. Whisper обучена на **680 тыс. часов мультиязычных данных (96 языков)** и демонстрирует передовое качество расшифровки на многих языках[\[1\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=OpenAI%20Whisper%3A%20Seq2Seq%20for%20A,utomatic%20Speech%20Recognition). Для русского языка Whisper показывает очень высокую точность по сравнению с предыдущими моделями (например, Vosk, Silero и др.), поэтому **Whisper Large** - приоритетный выбор для нашего сервиса распознавания. Альтернативы вроде моделей Silero STT (от компании Silero) могут работать быстрее на CPU для русского, но они существенно уступают Whisper в точности. Мы ориентируемся на максимальное качество, используя GPU, поэтому Whisper Large подходит лучше всего.

**Модель Whisper и дообучение:** Whisper - уже хорошо обученная модель, и специального дообучения под русский язык обычно не требуется. Однако могут возникать проблемы с **именами собственными** и специфическими терминами. Пользователь упоминал, что название продукта «RUMA» иногда распознается как «рум». У Whisper есть механизм подсказки контекста (prompt) - можно подать модели начальную фразу с правильным написанием терминов, чтобы повысить шанс корректного распознавания[\[2\]](https://cookbook.openai.com/examples/whisper_prompting_guide#:~:text=Whisper%20may%20incorrectly%20transcribe%20uncommon,can%20use%20prompts%20to). Однако более практичный путь - **постобработка**: после получения текста выполнять поиск и замену типичных ошибок (например, регекс \\bрум\\b → RUMA). В будущем возможно рассмотреть легкое дообучение (fine-tuning) модели на небольшом наборе данных, содержащих проблемные слова, либо внедрение словаря для biasing (некоторые энтузиасты предлагали подходы к "дополнению словаря" Whisper[\[3\]](https://github.com/openai/whisper/discussions/1268#:~:text=Introducing%20helping%20dictionnary%20with%20acronyms%2C,decide%20properly%20when%20in%20doubt)). На начальном этапе мы реализуем простую пост-правку расшифровки для исправления «RUMA» и других известных искажений.

**Загрузка модели Whisper:** Модель Whisper Large довольно объемная (~1.5 млрд параметров, ~3ГБ на диске, ~10ГБ VRAM в FP32). Для эффективного использования GPU: - Загрузить модель в половинной точности (FP16) - фреймворк transformers позволяет это сделать (параметр torch_dtype=torch.float16 при вызове from_pretrained). В FP16 модель займет ~5-6 ГБ видеопамяти, что вписывается в возможности современных GPU 10-16 ГБ. - Заранее загрузить модель **один раз при старте сервиса**, а не при каждом запросе. Инициализация модели - дорогая операция, поэтому гRPC-сервис должен держать модель в памяти. - Использовать ускорения: устройство CUDA, опция device_map="auto" или прямое .to("cuda") для отправки модели на GPU. Убедиться, что установлены необходимые зависимости (PyTorch >= 2.x, transformers и openai-whisper или whisper пакет). - Входные данные: перед распознаванием нужно прочитать WAV-файл и, при необходимости, привести к нужному формату (Whisper ожидает 16 kHz mono WAV). Если файл не в требуемом формате, можно воспользоваться ffmpeg или библиотекой librosa/torchaudio для конвертации. - Whisper сам сегментирует длинное аудио на куски ~30 секунд и обрабатывает их последовательно[\[4\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Whisper%20is%20trained%20on%2030,and%20maintaining%20manageable%20computational%20requirements)[\[5\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Performance%20on%20Long%20Audio%3A). Поэтому даже длительные записи (час и более) можно пустить в модель целиком - она внутренне обработает по частям. Однако нужно отслеживать использование памяти: для очень длинных записей может расти нагрузка (Whisper генерирует промежуточные буферы). - **Выход модели:** Whisper возвращает либо полный текст, либо список сегментов с таймкодами и текстом (при использовании трансформер-пайплайна с параметром return_timestamps=True или библиотеки whisperx). Мы можем получить **список сегментов** с временами и текстом. Это позволит нам впоследствии сопоставить сегменты с говорящими (по результатам диаризации).

**Итого:** рекомендуем **OpenAI Whisper Large-v2** для сервиса Transcribe. Он обеспечивает лучшую точность для русского языка. При реализации важно использовать GPU (CUDA) и загрузить модель единожды. Для корректности результатов - добавить шаг постобработки текста (например, заменить "рум" на "RUMA"). В перспективе можно улучшить модель либо через fine-tuning, либо используя функцию prompt bias (подать модель правильные термины в качестве подсказки), если это даст результаты.

## Диаризация (разметка говорящих) - обзор и выбор решения

**Назначение:** Диаризация отвечает на вопрос "кто когда говорил?"[\[6\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=Speaker%20diarization%20is%20the%20process,the%20transcription%20with%20speaker%20labels). На вход подается аудиозапись, а на выходе - список сегментов со временем начала/конца и меткой говорящего (спикера). Эта функция **обогащает транскрипт**: без нее мы знаем "что сказано", но не знаем "кто сказал"[\[6\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=Speaker%20diarization%20is%20the%20process,the%20transcription%20with%20speaker%20labels). Для встреч с несколькими участниками диаризация крайне важна, чтобы разделить реплики по персонам.

**State of the Art:** Современные системы диаризации бывают двух типов - _модульные (cascaded)_ и _энд-то-энд_. Энд-то-энд модели (например, **SortFORMER Diarizer** от NVIDIA) напрямую мапят аудио в метки говорящих[\[7\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=1.%20End). Однако они сложны в обучении и могут ограничивать число спикеров. Более распространены _каскадные_ подходы, где есть несколько шагов[\[8\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=2): 1. **VAD (Voice Activity Detection)** - определение моментов, где в аудио есть речь, а где пауза. 2. **Выделение эмбеддингов говорящих** - каждый фрагмент речи преобразуется в вектор, характеризующий голос (например, модели на основе ECAPA-TDNN). 3. **Кластеризация эмбеддингов** - группировка фрагментов по говорящим (чтобы одинаковые голоса получили одну метку). 4. (Опц.) **TS-VAD / MSDD** - более продвинутые нейросетевые способы уточнить разбиение, учитывая временной контекст (пример - Multiscale Diarization Decoder от NVIDIA).

В промышленности для диаризации часто применяют **Pyannote Audio** (от HuggingFace) и **NVIDIA NeMo**. Пользователь упомянул, что слышал про NeMo - действительно, **NVIDIA NeMo Toolkit** предлагает готовый конвейер для диаризации, сочетающий VAD и MSDD-модели. Подход, подтвержденный исследованиями - связка **Whisper + NeMo**: поскольку Whisper сам не умеет различать говорящих[\[9\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=translation%2C%20it%20does%20not%20inherently,VAD%29%20and), мы можем использовать инструменты NeMo для определения границ речевых фрагментов и их кластеризации по голосам[\[10\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=address%20this%20limitation%2C%20in%20our,VAD%29%20and)[\[11\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Now%2C%20let%E2%80%99s%20understand%20the%20Nemo,to%20find%20the%20occurrence%20of). Этот метод показал высокую точность: NeMo включает _легковесную модель VAD MarbleNet_ для разбивки по тишине и мощную модель **MSDD (Multi-Scale Diarization Decoder)** для определения спикеров, давая точную разметку по времени[\[11\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Now%2C%20let%E2%80%99s%20understand%20the%20Nemo,to%20find%20the%20occurrence%20of)[\[12\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=,msdd_model.diarize). В экспериментах такая связка Whisper+NeMo достигала очень низкой ошибки диаризации (DER около 5.8% на тесте)[\[13\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=print%28f%22Whisper%20Diarization%20Error%20Rate%3A%20%7Bwhisper_der%3A.2), что близко к специализированным сервисам.

Альтернатива - **Pyannote**: сообщество открытых решений предлагает предобученные модели (pyannote.audio), доступные через Hub HuggingFace. Они тоже реализуют VAD + эмбеддинги + кластеризацию под капотом. Pyannote широко известна качеством, но ее модели могут потребовать токена доступа и тоже достаточно тяжелые (модель эмбеддингов ~1GB, VAD поменьше).

Мы склоняемся к использованию **NVIDIA NeMo** для диаризации, учитывая корпоративный контекст и хорошие результаты. NeMo предоставляет готовые конфигурации _inference_ для разных сценариев - например, **«diar_msdd_telephonic»** для телефонных разговоров[\[14\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=match%20at%20L928%20,return%20config), что близко по характеру к встречам/переговорам (2-3 участника, четкая речь). Это можно взять за основу.

**Реализация сервиса Diarize:** Независимо от выбора (NeMo vs pyannote), на этапе старта GPU-сервиса нужно: - **Загрузить модели VAD и диаризации**. В случае NeMo - использовать nemo.collections.asr.models.\*\*ClusteringDiarizer\*\* или NeuralDiarizer API. Например, NeMo позволяет загрузить чекпойнты: VAD (vad_multilingual_marblenet), модель эмбеддинга (ecapa_tdnn или более новая) и MSDD (diar_msdd_telephonic). Можно воспользоваться готовым YAML-конфигом для пайплайна - NeMo имеет пример конфигурации и метод NeuralDiarizer.from_config(cfg)[\[12\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=,msdd_model.diarize). Такой подход автоматически выполнит VAD → эмбеддинги → MSDD. - **Оптимизация загрузки:** как и с Whisper, все тяжелые модели загружаются один раз. VAD и эмбеддинг - относительно легкие (VAD - небольшая CNN, эмбеддинг - ~100M параметров), а MSDD побольше. Все они помещаются в память GPU среднего уровня (несколько ГБ VRAM). - **Обработка запроса:** сервис получает путь к WAV-файлу, читает его (или прямо передает путь в NeMo, если поддерживается). Далее исполняется пайплайн: VAD отдает промежутки речи, эмбеддинги рассчитываются по этим промежуткам, затем MSDD выдаст финальную разбивку с метками. - **Результат:** формируем DiarizationResult - список сегментов {speaker, start_time, end_time}. Спикеров можно обозначать как **Speaker 1, Speaker 2** и т.д. (или Спикер 1 на русском, но лучше нейтрально). Количество говорящих можно не задавать заранее - алгоритм сам определит (MSDD может требовать min/max, но мы можем указать диапазон 1-N или конкретное если знаем число). Для большинства встреч разумно min_speakers=1, max_speakers=5 например. - **Аудио-предобработка:** Диаризация чувствительна к уровню шума и качеству звука. Если известно, что может быть фоновый шум, можно подумать о предварительной фильтрации. NeMo предлагает опцию отключения музыки/шума (в примере был флаг enable_stemming с Demucs для улучшения качества диаризации)[\[15\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=,of%20ram%20enable_stemming%20%3D%20True)[\[16\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=match%20at%20L568%20the%20Meta,using%20the%20original%20audio%20file). Пока можно не углубляться - начнем с базовой обработки. - **Pyannote (альтернатива):** В случае pyannote мы бы использовали Pipeline.from_pretrained("pyannote/speaker-diarization") (потребует токен) и вызвали pipeline(audio_file). Это тоже вернет объект с аннотациями по спикерам, который можно преобразовать в список сегментов. Pyannote, согласно некоторым тестам, достигает DER порядка 10-15% на реальных данных, что тоже приемлемо, но NeMo MSDD зачастую дает еще лучше при донастройке под домен.

**Выбор:** отталкиваясь от того, что пользователь уже знаком с NeMo, и эта библиотека активно поддерживается NVIDIA, **будем использовать NeMo**. Это требует установить nemo-toolkit\[all\] (довольно большой пакет ~4ГБ с весами) или загрузить необходимые части. Возможно, для начального прототипа, чтобы не утяжелять, начнем с pyannote (меньше зависимостей) - но с прицелом перейти на NeMo MSDD для продакшена. В **списке задач** отметим исследование и сравнение Pyannote vs NeMo, но реализацию выполним на том, что быстрее дадим результат (вероятно, pyannote pipeline, если получить доступ к весам, либо NeMo с готовой конфигурацией).

**Примечание по интеграции:** Диаризация сервису тоже передается _путь к аудио файлу_. Это значит, что **GPU-сервис должен иметь доступ к папке с файлами** (например, data/raw/). При разворачивании нужно либо монтировать общий volume, либо использовать сетевое хранилище. Это важно: без доступа к содержимому WAV-файла диаризация не сработает. (В альтернативной архитектуре можно было бы стримить аудио по gRPC, но мы придерживаемся подхода с общим хранилищем для упрощения.)

## Суммаризация встречи - модель и интеграция LLM

**Подход:** После получения полного текста расшифровки необходимо сгенерировать краткое **резюме встречи**. Здесь наиболее эффективно применение больших языковых моделей (LLM). В компании уже развернута собственная LLM с OpenAI-совместимым API - упоминается **Qwen-3** (вероятно, модель семейства Qwen от Alibaba). Предположительно, это мощная модель (сопоставима с GPT-3.5) и ее качества достаточно[\[17\]](https://github.com/QwenLM/Qwen3#:~:text=QwenLM%2FQwen3%20,and%20intelligent%20systems%20to%20date). Поэтому мы можем использовать существующий сервис: отправлять текст транскрипта на этот API и получать summary.

**Альтернативы:** Если по каким-то причинам внешний LLM API недоступен или требуется офлайн-решение, можно рассмотреть модели вроде **BART/RuGPT3** или специализированные модели суммаризации. Однако в открытом доступе нет явно превосходящих LLM решений для разговорных данных на русском. Есть модели для новостных или научных текстов (e.g. ruT5, mBART), но они не обучены на диалогах и показывают усредненные результаты. Практика показывает, что **универсальные LLM (GPT-3.5/4, Llama2, Qwen и т.д.)** дают очень хорошие суммаризации встреч, особенно если их правильно промптнуть (например: _"Суммируй следующую беседу, выдели ключевые решения и задачи..."_).

**План работы суммаризатора:** - **Вход:** полный транскрибированный текст встречи (объединение всех сегментов). - **Проблема контекста:** Длина расшифровки может быть большой (часовая встреча - десятки тысяч токенов). Нужно учесть ограничение контекстного окна модели. Если Qwen-3 имеет контекст ~8k токенов, а транскрипт больше - надо разбивать. Решение: **chunking + iterative summarization**. Например, разделить текст встречи на куски по ~5 минут речи и обрабатывать их последовательно. - **Стратегия суммаризации с чанками:** Можно просуммировать каждый кусок отдельно, а потом суммаризировать суммаризации (или просто объединить их). Либо использовать подход "sliding window": пройтись по тексту, постепенно генерируя сводку. Практический простой метод - разбить на N частей, получить N кратких саммари, затем отдать их модели для финального объединения. Такой многоэтапный процесс широко применяется, например: команда PressW описывает, как они **режут транскрипт на части, LLM обрабатывает куски, затем результаты объединяются**[\[18\]](https://pressw.ai/case-studies/llm-powered-automated-meeting-summarizer#:~:text=Our%20team%20wanted%20to%20be,read%20through%20at%20any%20time). - **Выход:** краткий текст (пару абзацев) с основными моментами встречи. Можно также дополнительно попросить LLM сгенерировать пункты действий или вопросы (как в примере PressW), но для начала достаточно общего резюме. - **Внедрение:** Наш Summarize сервис будет вызывать LLM API. Необходимо настроить URL и ключ API через переменные окружения (например, LLM_API_URL, LLM_API_KEY). Формат запроса - OpenAI API совместимый, т.е. JSON с полем prompt или messages. Qwen-3 совместим с ChatGPT API, значит, можно использовать стандартную библиотеку openai в Python, указав api_base URL компании и соответствующий ключ. - **Оптимизация:** Генерацию резюме можно делать **после завершения распознавания** (то есть не в реальном времени). Это совпадает с требованием - _"суммаризация только когда запись закончена"_. Мы будем запускать ее **последним этапом** pipeline, уже после диаризации. Скорость не критична - даже если резюме строится несколько секунд, это приемлемо, пользователь все равно получает его в конце. Если нужно ускорить, можно уменьшить объем входного текста (например, пропуская явные паузы или очень тривиальные фразы). - **Язык и стиль:** Для русскоязычных встреч логично генерировать резюме на русском. Надо в prompt явно указывать язык вывода. Qwen-3 должна поддерживать русский, но стоит проверить. Также определимся со стилем: возможно, деловой официальный тон. Это можно зафиксировать инструкцией в prompt. - **Качество:** Согласно опыту, LLM хорошо идентифицирует ключевые темы, но может потерять детали. Мы можем протестировать на примерах. Если качество **Qwen-3** нас устраивает (сказано, что устраивает), то этого достаточно. Если нет - можно попробовать более крупную модель (внешний GPT-4) или эксперименты с prompt (например, сначала попросить список тезисов, потом сформировать из них абзац).

**Примечание:** Суммаризация будет реализована как отдельный сервис **Summarize** с интерфейсом gRPC. Но технически, возможно, вместо выделенного GPU ей хватит CPU или обычного сервера, так как LLM вызывается по API. Однако для единообразия архитектуры (и т.к. в будущем можно разместить локальную модель на GPU) мы все равно оформим это как микросервис с gRPC. Он не будет требовать тяжелой инициализации модели, просто будет ретранслировать запрос к API. В перспективе, если захотим заменить внешнюю LLM на локальную (например, разместить Qwen-7B на нашей GPU-ноде), мы сможем это сделать внутри этого сервиса, прозрачно для остальной системы.

## Потоковая расшифровка (аудиострим) - долгосрочная перспектива

Пользователь упомянул **возможность потокового распознавания аудио (streaming)** для "вау"-эффекта. Это означает вывод транскрипции _в реальном времени_ по мере звучания речи, до завершения записи. Действительно, такие возможности есть (например, Whisper можно запустить на поток с небольшим буфером, или использовать специальные стриминговые модели ASR). Однако: - Реализовать это сложно: понадобятся механизмы буферизации аудио, VAD для отсылки порций в модель, и поддержка частичного вывода. Whisper в стандартном виде не оптимизирован для стриминга (он работает с 30-секундными фрагментами), но проекты как **WhisperX** или **NVIDIA Riva** могут обеспечить стриминг. - У нас пока нет такого требования на Phase 1; важнее получить правильный итоговый транскрипт и резюме по завершении записи. - Тем не менее, мы можем заложить **фундамент**: например, делать SSE-сообщения с частичными результатами (сейчас pipeline уже отдает сегменты Whisper по мере готовности через SSE). Это фактически имитирует стриминг после загрузки файла - UI видит текст постепенно. Для live-аудио (с микрофона) можно сделать в будущем: frontend будет отправлять аудио пакетами, а backend - реализует WebSocket или другой SSE, но это отдельный большой кусок работы.

Мы отнесем **стриминговое распознавание** в список будущих улучшений (низкий приоритет). В текущем подходе дождемся окончания записи (файл загружен целиком) и затем выполним все шаги офлайн. Такой режим проще для начального релиза и соответствует требованию: _"суммаризация точно только когда запись закончена"_.

Отметим, что даже без live-стриминга наш сервис уже работает асинхронно через SSE: после загрузки файла сразу начинается обработка, и пользователь видит прогресс (постепенно появляются расшифрованные фрагменты, затем диаризация, потом итоговое резюме). Это достаточно интерактивно.

## Архитектура GPU-сервисов и запуск

**Единый репозиторий:** Код GPU-сервисов мы расположим в том же репозитории, но, вероятно, в отдельной директории (например, gpu_services/). Хотя эти сервисы могут разворачиваться на другой машине или контейнере, хранение кода вместе упростит синхронизацию версий протоколов и тестирование.

**Микросервисы vs Монолит:** Каждая задача (ASR, Diarization, Summarization) может быть вынесена в отдельный сервис (контейнер). Это хорошо с точки зрения масштабирования - можно, скажем, запустить несколько копий ASR-сервиса, если он самый тяжелый, или разместить их на разных GPU. Изначально так и планировалось: в **docker-compose.gpu.yml** упомянуты три контейнера - asr, speaker, summarizer[\[19\]](https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/README.md#L21-L29). Однако сейчас клиентская часть (FastAPI) ожидает **один адрес** GPU (пара GPU_GRPC_HOST:PORT). Чтобы три сервиса заработали, есть два варианта: 1. **Единый gRPC-сервер**: объединить три сервиса в одном приложении (т.е. один процесс, регистрирующий 3 gRPC сервиса на одном порту). gRPC это поддерживает - можно объявить TranscribeServicer, DiarizeServicer, SummarizeServicer и все добавить на один grpc.Server. Тогда GPU_GRPC_HOST/PORT указывает на этот сервер, и на нем доступны три метода. Этот подход проще для конфигурации (ничего менять не надо в Settings), но сложнее в коде (один сервис будет загружать и Whisper, и NeMo, и занимать много ресурсов). 2. **Отдельные сервисы/адреса**: запустить 3 разных сервера (например, на портах 50051, 50052, 50053). Тогда нам придется научить FastAPI использовать разные хосты/порты для каждого типа клиента. Сейчас фабрика create_grpc_client не умеет этого - она берет один GPUSettings. Можно расширить GPUSettings полями для каждого сервиса (напр. ASR_HOST/PORT, DIAR_HOST/PORT, ...), или задать конвенцию портов (например, тот же хост, порты +1 +2). Это потребует небольших изменений, но выполнимо.

С точки зрения **развертывания**, вероятно правильнее _отдельные контейнеры_. Тогда можно даже на разных машинах их держать (например, ASR и diarization требуют GPU, а summarizer может на CPU). Чтобы при этом не усложнять сейчас код, возможен компромисс: **сделать единый сервер для dev**, а потом расширить для прод. Но лучше сразу заложить гибкость.

Мы поступим так: - Реализуем код сервисов модульно (файлы transcribe_server.py, diarize_server.py, summarize_server.py). Каждый из них сможет запускаться отдельно (слушая свой порт). - Добавим в .env возможность задавать разные порты (например, GPU_ASR_PORT, GPU_DIAR_PORT, GPU_SUMM_PORT). По умолчанию можно делать 50051, 50052, 50053. - В GPUSettings можно пока оставить один хост/порт, а порты для других считать как сдвиг (или добавить параметры). - Скорректируем create_grpc_client: например, если service == 'diarize', порт = базовый + 1, и т.д. Или лучше сразу передавать конкретный адрес.

В **первой итерации** можем, однако, запустить всё на одном сервере (в одном процессе) - это упростит отладку. Для этого в коде сервиса можно зарегистрировать все три сервиса на один grpc.Server(). Этот "монолитный" GPU-сервис будет занимать один GPU и выполнять все. Если GPU мощный (например, 24GB), он потянет и Whisper, и NeMo одновременно. Но в будущем, при росте нагрузки, вероятно разделим. Поэтому в задачах отметим и вариант объединенного сервера, и подготовку к разделению.

**Docker и GPU:** Для запуска сервисов на GPU в Docker Compose нужно: - Использовать флаг --gpus all (или в docker-compose.yml указать deploy.resources.reservations.devices с GPU). Мы настроим infra/docker-compose.gpu.yml соответствующе. - Образ для сервисов - сделаем свой Dockerfile, на базе, например, nvidia/cuda:12.2.0-runtime-ubuntu20.04 + Python, устанавливаем PyTorch с CUDA, huggingface, nemo, etc. Это образ будет общий или можно раздельно оптимизировать (например, summarizer не требует NeMo, а diarize не требует transformers). Но для простоты можно один Dockerfile для все трех (они будут из одного образа, просто запускаются с разными командами). - Смонтировать том с аудио: - ./data/raw:/app/data/raw для всех контейнеров, чтобы пути совпадали. - Прописать сеть, чтобы backend мог обращаться по имени сервисов (e.g. asr:50051 etc). - Сертификаты TLS: если будет mTLS, надо прокинуть файлы ключей в контейнеры и настроить серверы использовать их (grpc-сервер с SSL). Пока можно ограничиться insecure (VPN-туннель защищает). Но оставим возможность TLS - backend уже умеет проверять, нужно чтобы серверы тоже могли принять mTLS. Вернемся к этому, когда реальные сертификаты будут.

**Обработка ошибок и таймауты:** Интеграция с реальными сервисами требует учесть нештатные ситуации: что, если распознавание или диаризация займет слишком долго или упадет? FastAPI SSE у нас работает асинхронно, но потенциально долго (минуты). Надо: - Задать **таймаут** на стороне gRPC клиента? grpc-aio позволяет указать timeout при вызове. Можно установить, например, 5 минут на транскрипцию, 2 минуты на диаризацию, 1 минуту на суммари. Либо оставить по умолчанию (бесконечно) и просто ждать. - Если сервис вернул ошибку (exception), gRPC передаст код. Наш pipeline сейчас не обрабатывает ошибки - стоит добавить try/except вокруг вызовов .run() и .stream_run() и в случае ошибки логировать и пробрасывать событие типа error в SSE, а также менять статус встречи на "failed". Добавим это как задачу.

**Тестирование:** Нам понадобятся интеграционные тесты, хотя бы вручную: после поднятия сервисов на GPU, прогнать файл через /upload и проверить, что мы получили осмысленную расшифровку с метками и резюме. Автоматизировать можно на коротком аудио (несколько секунд, 1-2 фразы) - чтобы и Whisper, и диаризация отработали быстро. Для этого можно включить такой файл в репозиторий (или генерировать). В CI, конечно, мы не сможем запустить GPU, но локально разработчик сможет проверить.

Далее приведены конкретные **инструкции и задачи** для реализации GPU-сервисов и интеграции.

## Задачи: Реализация сервисов обработки (TODO_GPU.md)

Ниже перечислены задачи для создания и настройки трех GPU-сервисов: ASR, Diarization, Summarization. Формат аналогичен существующему TODO: каждая задача имеет ID, статус, описание, область, приоритет и зависимости.

**Важно:** На момент начала работ папка для GPU-сервисов пуста - нужно создать структуру. Предлагается завести каталог gpu_services/ в корне, где будут подпапки или файлы для каждого сервиса, а также общий код (например, утилиты, модели, Dockerfile).

Приоритеты: P0 - критически важно для запуска основной функциональности, P1 - важные улучшения, P2 - дополнительные (не блокирующие).

### TODO_GPU.md (задачи по GPU-сервисам)

| ID  | DONE | Task (Описание задачи) | Area | Priority | Dependencies |
| --- | --- | --- | --- | --- | --- |
| **G1** | ✗   | **ASR gRPC Service (Whisper Large-v2)** - Реализовать сервис распознавания речи. Создать модуль (например, gpu_services/asr_service.py) с классом TranscribeServicer, имплементирующим протокол gRPC из transcribe.proto. В конструкторе загрузить модель Whisper Large-v2 через HuggingFace Transformers (с torch_dtype=float16, на GPU). Реализовать метод Run(AudioRequest) - открыть WAV-файл по пути из request.path (файл доступен на общем томе data/raw), выполнить распознавание (в виде текста **и сегментов**). В ответ вернуть либо полный текст, либо список сегментов (решить, что удобнее: если сегментов мало, можно вернуть массив {text, start, end}; Whisper выдает сегменты ~30 секунд с микро-паузами). **Важно**: включить пост-обработку - скрипт замены известных ошибок (например, «рум» -> «RUMA»). Убедиться, что сервис однопоточно обрабатывает запросы (Whisper потребляет почти весь GPU). Написать логирование времени распознавания. | GPU/ASR | P0  | protos, model weights ready |
| **G2** | ✗   | **Speaker Diarization gRPC Service** - Создать сервис диаризации (gpu_services/diarize_service.py) с классом DiarizeServicer. Загрузка модели: интегрировать **NVIDIA NeMo** (или альтернативно pyannote) для спикер-диаризации. На старте загрузить необходимые модели: VAD, speaker embedding и MSDD (multi-scale diarizer) по конфигу или отдельным чекпойнтам. Например, с помощью NeMo nemo.collections.asr.models.ClusteringDiarizer или NeuralDiarizer: подготовить YAML (можно использовать готовый diar_inference.yaml для «телефонного» сценария)[\[14\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=match%20at%20L928%20,return%20config), подгрузить через cfg = OmegaConf.load(...) и model = NeuralDiarizer(cfg). Метод Run(AudioRequest): получить request.path, выполнить полную диаризацию на этом файле. Результат - сформировать DiarizationResult со списком сегментов: {speaker_label, start_time, end_time}. Присваивать говорящих как **Speaker 1, Speaker 2, …** в порядке появления (NeMo обычно так и индексирует). Если обнаружен только один спикер, вернуть один сегмент (или можно вообще вернуть пусто? - но лучше один сегмент "Speaker 1: весь файл"). Учесть, что длительность выполнения может быть значительной (например, 1 минута на час аудио) - при необходимости, логировать прогресс или использовать более легкую модель на коротких тестах. | GPU/Diarization | P0  | protos, ASR output format |
| **G3** | ✗   | **Summarization gRPC Service (LLM)** - Реализовать сервис суммаризации (gpu_services/summarize_service.py) с классом SummarizeServicer. Этот сервис не загружает тяжелых локальных моделей, вместо этого будет обращаться к внешнему API (OpenAI-compatible). В методе Run(TextRequest), где request.text - полный транскрипт встречи, реализовать логику: если текст очень большой (напр. > ~5000 слов), выполнить разбиение на части и вызвать LLM несколько раз. Сам простой вариант: разделить текст на куски по ~N символов (ориентируясь на ~4096 токенов), для каждого вызвать LLM с промптом "Суммируй этот фрагмент…". Затем результаты склеить и еще раз отправить LLM для финальной суммаризации. Возможно, сразу попробовать отправить весь текст, если он помещается в контекст (например, Qwen3 допускает 8k токенов). Вызов LLM: использовать библиотеку openai или httpx POST запрос. Конфигурация через env: например, LLM_API_BASE, LLM_API_KEY. Параметры: выбрать модель (если требуется, например, "qwen-7b" или через совместимый endpoint), температуру низкую (0.2-0.3 для консистентности). Полученный summary текст вернуть в поле text или summary ответа. Добавить обработку ошибок: если API недоступно или вернуло ошибку - логировать и кидать исключение gRPC. Убедиться, что кодировка (русский язык) сохраняется. | GPU/Summarization | P0  | protos, ASR output ready |
| **G4** | ✗   | **Dockerization of GPU Services** - Написать Dockerfile(ы) для новых сервисов. Можно единый Dockerfile в корне gpu_services/ для всех, основанный на официальном образе **Python 3.10+ with CUDA** (например, nvidia/cuda:12.2.0-runtime-ubuntu20.04 + установка Python). В Dockerfile установить: pip install torch torchvision torchaudio --extra-index-url <https://download.pytorch.org/whl/cu118>, pip install transformers==4.x, pip install git+<https://github.com/openai/whisper.git>, pip install nemo_toolkit\[all\] (или минимально nemo_toolkit\[asr\]), а также openai (если нужен) или requests для API, protobuf, grpcio и grpcio-tools (для сервисов). Размер образа будет большой, но допустимо для GPU node. В docker-compose.gpu.yml, прописать три службы: asr, speaker, summarizer, каждая запускает контейнер из этого образа, но с разными командами (например, python -m gpu_services.asr_service, и аналогично для других). Прокинуть ports: например, 50051:50051 для asr, 50052 для speaker, 50053 для summarizer. Смонтировать volumes: ./data/raw внутрь контейнеров по тому же пути (чтобы AudioRequest.path был валиден внутри). Если используем TLS, смонтировать и сертификаты. Проверить, что контейнеры видят GPU: добавить deploy: { resources: { reservations: { devices: \[{ capabilities: \[gpu\] }\] } } } или использовать флаг --gpus all в командной строке. После написания Dockerfile, обновить документацию (README) по запуску GPU-compose. | DevOps/Container | P0  | G1, G2, G3 (code complete) |
| **G5** | ✗   | **Accuracy Tuning (RUMA & rare words)** - _Research/Implement improvement for domain-specific terms._ Проанализировать качество распознавания на целевом домене: убедиться, что название **RUMA** и другие специфичные слова распознаются правильно. Если продолжаются ошибки, реализовать механизм их исправления. Начать с простого: составить список слов-исключений и их корректных написаний, добавить функцию замены в пост-обработке ASR (из задачи G1). Опционально, изучить возможность использования **Whisper prompting** - передавать модель контекст (например, "Словарь: RUMA.")[\[2\]](https://cookbook.openai.com/examples/whisper_prompting_guide#:~:text=Whisper%20may%20incorrectly%20transcribe%20uncommon,can%20use%20prompts%20to). Также оценить, нужно ли fine-tuning: OpenAI не выпустила открытых средств для fine-tune Whisper, но можно обучить небольшую модель-шепотку (например, на базе Nvidia NeMo ASR) для добавочной коррекции, что вряд ли оправдано. Задача считается выполненной, когда тест на нескольких записях подтверждает: "RUMA" в выводе пишется правильно, и другие часто искаженные термины тоже. Документировать принятый подход (в README или комментариях к коду). | GPU/ASR Quality | P1  | G1 (baseline ready) |
| **G6** | ✗   | **Pyannote vs NeMo comparison** - _(Optional research)_ Провести сравнение качества и производительности диаризации с помощью Pyannote Audio. Для научной полноты, попробовать интегрировать pyannote pipeline: установить pip install pyannote.audio и соответствующие модели (например, pyannote/speaker-diarization@2023.07 - может потребовать hf токен). Запустить на тех же тестовых аудио, сравнить метки и ошибки с версией NeMo. Отметить, сколько времени занимает каждый, сколько памяти потребляет. Если Pyannote дает сопоставимое качество и проще в установке, можно на начальном этапе использовать ее (особенно если проблемы с NeMo). Результаты эксперимента описать в docs/researches/speech_stack_research.md (или аналогичном документе). Эта задача нацелена на долгосрочный выбор оптимального решения; она не блокирует основной функционал, поэтому низкий приоритет. | GPU/Diarization | P3  | G2 (initial diarization) |
| **G7** | ✗   | **Unified GPU Service (optional mode)** - _(Dev/Testing convenience)_ Реализовать возможность запускать **единый** сервис, обслуживающий сразу все три типа запросов. Например, файл gpu_services/unified_server.py который импортирует классы TranscribeServicer, DiarizeServicer, SummarizeServicer, регистрирует их на одном grpc.Server() (каждый с соответствующим protobuf-servicer). Порт взять из GPU_GRPC_PORT. Этот режим удобен для локального запуска (например, на машине разработчика с одной GPU) вместо поднятия трех контейнеров. Он также упростит отладку: можно проверить цепочку end-to-end внутри одного процесса. Добавить переключатель, например, через переменную окружения GPU_SINGLE_PROCESS=true использовать unified сервер. В документации описать, что в продакшене рекомендуется разносить сервисы, но на dev можно так. Реализация: главное следить, чтобы загрузка моделей не съела всю память (Whisper+NeMo вместе). Если не поместятся на одной 16GB карте, можно отказаться от unified. Задача более для удобства разработки. | GPU/All-in-One | P2  | G1, G2, G3 (basics done) |

## Задачи: Интеграция GPU-сервисов в систему (TODO_INTEGRATION.md)

Теперь, когда определены задачи по созданию самих сервисов, необходимо спланировать интеграцию их с основным приложением (FastAPI бэкендом) и фронтендом. Эти задачи охватывают изменения конфигурации, оркестрацию, пост-обработку результатов и обновление пользовательского опыта (например, отображение говорящих). Итоговый список оформлен как отдельный TODO_INTEGRATION.md.

**Цели интеграции:** - Переключить Backend с mock-клиентов на реальные gRPC вызовы. - Обеспечить доступность GPU-сервисов (настройка адресов, безопасность). - Обработать новые данные (например, сегменты диаризации) - возможно, сохранять их и использовать на фронтенде. - Улучшить выдачу для пользователя: имена говорящих в тексте, и т.д. - Оставить возможность работать в режиме разработки без GPU (т.е. fallback на mock).

Приоритеты интеграционных задач также размечены: P0 для критически необходимых, P1 для важных улучшений, P2/P3 для дальнейших шагов.

### TODO_INTEGRATION.md (задачи по интеграции)

| ID  | DONE | Task (Описание задачи) | Area | Priority | Dependencies |
| --- | --- | --- | --- | --- | --- |
| **I3** | ✗   | **Switch to Real GPU Services** - Перевести бэкенд на использование реальных gRPC-сервисов вместо mock. В модуле app/services/pipeline.py изменить дефолт GRPC_CLIENT_TYPE на 'grpc' (сейчас по умолчанию 'mock'). Убедиться, что при старте приложения загружается GPUSettings из .env с правильными GPU_GRPC_HOST и GPU_GRPC_PORT. Если мы реализовали раздельные порты для сервисов (см. G4), нужно обновить фабрику create_grpc_client: например, в mapping_real можно хранить не только класс, но и порт. Либо проще - перед созданием клиентов вызывать GPUSettings() несколько раз с разными портами (не самый изящный способ). Возможно, лучше расширить GPUSettings: добавить поля asr_port, diar_port, summ_port (и host общий). Затем create_grpc_client(service, gpu_settings) будет подставлять соответствующий порт. Реализовать эту логику. После переключения убедиться, что при вызове /api/meeting/stream реально идут запросы на GPU-ноду (это видно по логам или даже можно временно добавить вывод: какой клиент используется). Обеспечить fallback: например, для среды тестирования, где нет GPU, оставить возможность GRPC_CLIENT_TYPE=mock. Это уже заложено, главное - не сломать. | Backend/GPU Integration | P0  | G1, G2, G3 (services up) |
| **I4** | ✗   | **Volume Sharing for Audio** - Настроить совместимый доступ к аудиофайлам между бекендом и GPU-сервисами. В локальной среде (docker-compose) это означает убедиться, что контейнер backend и контейнеры asr/diar/summ монтируют одну и ту же директорию data/raw. В продакшене - решить, как будет организовано хранение: возможно, использовать распределенное хранилище (NFS, S3). На первое время можно считать, что GPU-ноде доступна папка с тем же содержимым (например, через NFS mount). Задача: задокументировать это требование и проверить: загрузить файл через API, убедиться, что внутри контейнера asr по тому же пути файл существует. Если нет - поправить путь (в крайнем случае, можно передавать аудио по gRPC streaming, но это усложнение, стараемся избежать). В .env.example указать, что путь RAW_AUDIO_DIR должен соответствовать тому, что видят GPU. **Конкретно**, возможно, лучше хранить RAW_AUDIO_DIR=/app/data/raw внутри контейнеров - тогда и backend, и GPU-сервисы будут использовать одинаковый /app/data/raw/.... Исправить конфигурацию, если сейчас по-другому. | Infra/Storage | P0  | deployment, G1-G3 |
| **I5** | ✗   | **Secure gRPC Connection (TLS/mTLS)** - Настроить безопасное соединение между бекендом и GPU-сервисами. Согласно docs/gpu_security.md, два варианта: VPN или mTLS. Если у нас Docker Compose локально, можно оставить insecure. Но в боевой среде, вероятно, будет VPN-туннель, либо потребуется включить mTLS. Подготовить инфраструктуру для mTLS: сгенерировать самоподписанные сертификаты для сервера и клиента (например, с CN=GPU Server). Настроить GPU-сервисы (grpc-сервер) использовать этот сертификат: при создании grpc.Server() применять grpc.ssl_server_credentials(...) с серверным ключом/сертом и опционально требованием клиентского аутентификата (для mTLS). На стороне бекенда GPUSettings уже поддерживает указание путей GPU_GRPC_TLS_\*. Провести локальный тест: включить GPU_GRPC_USE_TLS=true, настроить пути на файлы (нужно поместить их в контейнеры, напр. в certs/). Проверить, что соединение устанавливается (в случае ошибки будет лог TLS handshake). После проверки - описать в документации шаги генерации сертификатов и переменные. Эта задача важна для безопасности, но может быть выполнена после базовой интеграции, поэтому P1. | Backend/GPU Security | P1  | G1-G4, I3 (basic working) |
| **I6** | ✗   | **Persist & Expose Speaker Labels** - Интегрировать результаты диаризации в хранилище и в API. Сейчас транскрипты сохраняются постфактум (Task S2) - каждый сегмент речи в Transcript таблице. Нужно дополнить модель Transcript (или Meeting) полем для спикера. В базе у нас Transcript имеет, кажется, speaker или speaker_id (в планах был Transcript.speaker_id). Если нет - можно добавить (миграцией) поле speaker_label VARCHAR или числовой id + отдельную таблицу Speakers. Для упрощения пока можно хранить прямо метку "Speaker 1". Реализовать: в TranscriptService.stream_transcript после завершения всех шагов (или по ходу) - сопоставить сегменты расшифровки с сегментами диаризации. Whisper сегменты имеют таймстампы, диаризация - интервалы. Алгоритм: для каждого текстового сегмента взять midpoint (среднее время), найти в списке диаризации сегмент, куда этот time попадает, и назначить его speaker_label. Или более грубо: если Whisper сегменты короткие, можно по началу совпадение. После этого, при сохранении Transcript в БД (Task S2), сохранять speaker_label в запись. Дополнительно, обновить ответ API /meeting/{id} - сейчас он возвращает просто текст; нужно возвращать вместе с говорящим. Можно изменить формат: например, массив объектов {speaker: "Speaker 1", text: "фраза..."}. Фронтенду тогда проще отобразить с разделением по спикерам (например, разным цветом). Также в SSE можно сразу в событиях transcribe передавать сегменты с указанием спикера, но у нас диаризация приходит позже. Вероятно, проще: фронт, получив события diarize, сам объединит (но это сложно). Поэтому лучше на бэке после полной обработки подготовить **финальный массив** и отдавать его по запросу детали встречи (а SSE оставить как есть для live feedback). Задача включает изменение модели, репозитория, схемы ответа - все, чтобы **спикеры были видны пользователю** хотя бы в итоговом просмотре. | Backend/API & DB | P1  | S2, G2, I3 |
| **I7** | ✗   | **Frontend Speaker Labels UI** - Обновить фронтенд для отображения говорящих. После задачи I6, backend будет предоставлять информацию о спикерах. Нужно использовать ее на UI: например, в компоненте Live Transcript (F8) пока мы просто добавляем текст строки. Когда приходит событие type: diarize с payload сегмента, можно воспользоваться им: хотя тут tricky - у нас текст уже вывели. Возможно, лучше пока не пытаться в реальном времени метить (слишком сложно), а сделать в **Transcript Detail View (F10)**: при запросе полного транскрипта (который теперь содержит спикеров) отобразить каждый сегмент с именем. В списке встреч (F9) можно выводить кратко, кто участники (например, "Спикеры: 2"). Это можно определить из меток (множество speaker_label). Конкретные изменения: компонент TranscriptDetail должен парсить поле transcripts из ответа - если объект включает speaker, отрендерить как &lt;b&gt;Speaker 1:&lt;/b&gt; текст. Также можно стилизовать (разным цветом для разных спикеров - по label). Live view: можно оставить как есть (без спикеров) или попытаться улучшить: например, когда приходит первый сегмент Speaker 1, отобразить его с тегом, а далее, пока не сменится спикер, печатать в тот же блок. Это сложно в текущем SSE потоке. Возможно, на первом этапе UI live не будет показывать метки, а только итог - это допустимо. Обновить локализацию, если нужно (Speaker -> Спикер). Протестировать на встрече с 2 участниками, убедиться, что цвета/метки понятны. | Frontend/UI | P2  | I6 (backend data ready) |
| **I8** | ✗   | **End-to-End Load Test & Optimization** - После внедрения GPU-сервисов, провести сквозное тестирование на реалистичных данных. Взять аудиозапись встречи (например, 5-10 минут, 2 говорящих), загрузить через фронт, проследить: (a) распознавание работает и результат разумный, (b) говорящие определились верно, (c) суммаризация адекватна содержимому. Измерить время выполнения каждого этапа (можно добавить таймеры логов: "ASR took X sec, Diarization Y sec, Summary Z sec"). Если суммарно на 10 мин аудио уходит более, скажем, 2-3 минут - это ок для оффлайн режима. Если слишком долго, подумать, где узкое место: Whisper Large может ~реальном времени (x1) расшифровывать на сильной GPU, диаризация NeMo обычно быстрая (<0.5x), суммаризация - секундами. Если выявлены проблемы (например, слишком длинные паузы между SSE событиями, или фронт может дать таймаут), оптимизировать: можно разбивать аудио на куски параллельно (сложно с Whisper), либо использовать более маленькую модель (Whisper Medium) в ущерб качеству. Обсудить с командой допустимый компромисс. Результаты теста оформить в виде комментариев в QUESTIONS.md или в отдельном отчете. Цель - убедиться, что система готова к демо: работает стабильно, пусть и без масштабирования. | System/Test | P1  | I3, I4, I6 (full pipeline) |
| **I9** | ✗   | **Streaming Audio Roadmap** - _(Для планирования)_ Сформировать план добавления поддержки **on-the-fly** распознавания (streaming) в будущем. Включает: выбор технологии (возможно, использовать модель **NVIDIA Riva** или **Whisper streaming** вариант), изменение протокола (gRPC streaming вместо отправки пути), обновление фронтенда (запись аудио chunk-ами). Описать, какие шаги нужны: например, 1) внедрить WebSocket или WebRTC передачу аудио с клиента на сервер, 2) на сервере организовать буфер и отправку в ASR сервис порциями, 3) модификация ASR сервиса на прием stream (Whisper можно каждые N секунд запускать или использовать VAD). Уделить внимание синхронизации: как убедиться, что итоговый текст не дублируется и не теряется. Так как задача исследовательская, поместить ее результат в документацию (например, docs/researches/streaming_plan.md). Эта задача не приводит к немедленному коду, но важна, чтобы оценить трудоемкость и требования (вдруг понадобится другая модель). В рамках текущего TODO, достаточно написать план и отложить реализацию. | Research/Future | P3  | (none, planning task) |

**Примечание:** Нумерация интеграционных задач продолжает ID из предыдущего списка (Part 2) - предыдущие инфратаски дошли до I2, поэтому начинаем с I3. После выполнения этих задач система должна полноценно работать на GPU-сервисах с сохранением транскриптов, что соответствует завершению **Фазы 2 (Реальная обработка аудио)** из плана[\[20\]](https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/implementation_plan.md#L130-L137). Далее можно будет переходить к Фазе 3 (UI улучшения) и другим фазам, уже имея надежный backend-функционал.

[\[1\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=OpenAI%20Whisper%3A%20Seq2Seq%20for%20A,utomatic%20Speech%20Recognition) [\[4\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Whisper%20is%20trained%20on%2030,and%20maintaining%20manageable%20computational%20requirements) [\[5\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Performance%20on%20Long%20Audio%3A) [\[9\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=translation%2C%20it%20does%20not%20inherently,VAD%29%20and) [\[10\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=address%20this%20limitation%2C%20in%20our,VAD%29%20and) [\[11\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=Now%2C%20let%E2%80%99s%20understand%20the%20Nemo,to%20find%20the%20occurrence%20of) [\[12\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=,msdd_model.diarize) [\[13\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=print%28f%22Whisper%20Diarization%20Error%20Rate%3A%20%7Bwhisper_der%3A.2) [\[14\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=match%20at%20L928%20,return%20config) [\[15\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=,of%20ram%20enable_stemming%20%3D%20True) [\[16\]](https://learnopencv.com/automatic-speech-recognition/#:~:text=match%20at%20L568%20the%20Meta,using%20the%20original%20audio%20file) Whisper Automatic Speech Recognition (ASR) with Diarization

<https://learnopencv.com/automatic-speech-recognition/>

[\[2\]](https://cookbook.openai.com/examples/whisper_prompting_guide#:~:text=Whisper%20may%20incorrectly%20transcribe%20uncommon,can%20use%20prompts%20to) Whisper prompting guide | OpenAI Cookbook

<https://cookbook.openai.com/examples/whisper_prompting_guide>

[\[3\]](https://github.com/openai/whisper/discussions/1268#:~:text=Introducing%20helping%20dictionnary%20with%20acronyms%2C,decide%20properly%20when%20in%20doubt) Introducing helping dictionnary with acronyms, people names, place ...

<https://github.com/openai/whisper/discussions/1268>

[\[6\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=Speaker%20diarization%20is%20the%20process,the%20transcription%20with%20speaker%20labels) [\[7\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=1.%20End) [\[8\]](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html#:~:text=2) Speaker Diarization - NVIDIA NeMo Framework User Guide

<https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/intro.html>

[\[17\]](https://github.com/QwenLM/Qwen3#:~:text=QwenLM%2FQwen3%20,and%20intelligent%20systems%20to%20date) QwenLM/Qwen3 - GitHub

<https://github.com/QwenLM/Qwen3>

[\[18\]](https://pressw.ai/case-studies/llm-powered-automated-meeting-summarizer#:~:text=Our%20team%20wanted%20to%20be,read%20through%20at%20any%20time) LLM powered automated meeting summarizer - PressW AI Solutions

<https://pressw.ai/case-studies/llm-powered-automated-meeting-summarizer>

[\[19\]](https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/README.md#L21-L29) README.md

<https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/README.md>

[\[20\]](https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/implementation_plan.md#L130-L137) implementation_plan.md

<https://github.com/kvcop/Voicerec-By-Codex/blob/c2404c94eebfea3600b8605981a0685f6304c029/docs/implementation_plan.md>
